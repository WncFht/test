---
title: 项目实践
date: 2024-11-21T09:02:44+0800
modify: 2024-12-06T00:15:22+0800
categories: graph
dir: Obsidian
share: true
tags:
  - graph
---

# 深度学习研究实践指南 - Part 1

## 环境配置与神经网络基础组件实现

### 一、开发环境搭建

#### 1. Python 环境配置

```bash
# 1. 安装Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh

# 2. 创建虚拟环境
conda create -n dl-research python=3.9
conda activate dl-research

# 3. 安装基础依赖
pip install numpy pandas matplotlib jupyter
```

#### 2. 深度学习框架安装

```bash
# 1. 安装PyTorch (根据你的CUDA版本选择)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# 2. 安装辅助工具
pip install wandb tensorboard pytest pylint black
```

#### 3. IDE 配置

推荐使用 PyCharm 或 VSCode，以下以 VSCode 为例：

1. 安装 Python 扩展
2. 配置代码质量工具：

```json
{
    "python.linting.pylintEnabled": true,
    "python.linting.enabled": true,
    "python.formatting.provider": "black",
    "editor.formatOnSave": true,
    "python.testing.pytestEnabled": true
}
```

#### 4. 代码质量工具配置

创建 `setup.cfg` :

```ini
[pylint]
max-line-length = 100
disable = C0111,C0103

[tool:pytest]
python_files = test_*.py
python_classes = Test
python_functions = test_*
```

### 二、项目一：神经网络基础组件实现

#### 1. 项目结构

```
neural_lib/
├── core/
│   ├── __init__.py
│   ├── layers.py
│   ├── activations.py
│   ├── losses.py
│   └── optimizers.py
├── tests/
│   ├── __init__.py
│   ├── test_layers.py
│   ├── test_activations.py
│   └── test_losses.py
├── examples/
│   ├── mnist_mlp.py
│   └── gradient_check.py
├── requirements.txt
└── README.md
```

#### 2. 核心组件实现

##### 2.1 基础层实现 (layers. py)

```python
import numpy as np

class Layer:
    def __init__(self):
        self.params = {}
        self.grads = {}

    def forward(self, x):
        raise NotImplementedError

    def backward(self, dout):
        raise NotImplementedError

class Linear(Layer):
    def __init__(self, in_features, out_features):
        super().__init__()
        # 使用He初始化
        scale = np.sqrt(2.0 / in_features)
        self.params['W'] = scale * np.random.randn(in_features, out_features)
        self.params['b'] = np.zeros(out_features)
        
        self.x = None

    def forward(self, x):
        """
        前向传播
        Args:
            x: 输入数据 (N, d1)
        Returns:
            out: 输出数据 (N, d2)
        """
        self.x = x
        out = np.dot(x, self.params['W']) + self.params['b']
        return out

    def backward(self, dout):
        """
        反向传播
        Args:
            dout: 输出梯度 (N, d2)
        Returns:
            dx: 输入梯度 (N, d1)
        """
        self.grads['W'] = np.dot(self.x.T, dout)
        self.grads['b'] = np.sum(dout, axis=0)
        dx = np.dot(dout, self.params['W'].T)
        return dx
```

##### 2.2 激活函数 (activations. py)

```python
class ReLU(Layer):
    def __init__(self):
        super().__init__()
        self.mask = None

    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0
        return out

    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        return dx

class Sigmoid(Layer):
    def __init__(self):
        super().__init__()
        self.out = None

    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * self.out * (1 - self.out)
        return dx
```

##### 2.3 损失函数 (losses. py)

```python
class MSELoss:
    def __init__(self):
        self.x = None
        self.t = None
        
    def forward(self, x, t):
        """
        Args:
            x: 预测值 (N, d)
            t: 目标值 (N, d)
        """
        self.x = x
        self.t = t
        return 0.5 * np.sum((x - t) ** 2) / x.shape[0]
        
    def backward(self):
        dx = (self.x - self.t) / self.x.shape[0]
        return dx

class CrossEntropyLoss:
    def __init__(self):
        self.y = None  # 输出概率
        self.t = None  # 目标值
        
    def forward(self, x, t):
        """
        Args:
            x: 预测值 (N, C)
            t: 目标值 one-hot编码 (N, C)
        """
        self.t = t
        self.y = self._softmax(x)
        log_likelihood = np.log(self.y[np.arange(x.shape[0]), t.argmax(axis=1)])
        loss = -np.sum(log_likelihood) / x.shape[0]
        return loss
        
    def backward(self):
        dx = (self.y - self.t) / self.t.shape[0]
        return dx
        
    def _softmax(self, x):
        x = x - np.max(x, axis=1, keepdims=True)  # 防止指数爆炸
        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)
```

#### 3. 测试代码

##### 3.1 层测试 (test_layers. py)

```python
import numpy as np
import pytest
from core.layers import Linear

def test_linear_layer():
    # 测试前向传播维度
    layer = Linear(3, 2)
    x = np.random.randn(4, 3)
    out = layer.forward(x)
    assert out.shape == (4, 2)
    
    # 测试反向传播维度
    dout = np.random.randn(4, 2)
    dx = layer.backward(dout)
    assert dx.shape == (4, 3)
    assert layer.grads['W'].shape == (3, 2)
    assert layer.grads['b'].shape == (2,)
    
    # 测试梯度计算
    def f(x): return np.sum(layer.forward(x))
    def relative_error(x, dx_num, dx):
        return np.max(np.abs(dx_num - dx) / (np.abs(dx_num) + np.abs(dx)))
        
    # 数值梯度
    h = 1e-7
    x_num = x.copy()
    dx_num = np.zeros_like(x)
    for i in range(x.size):
        tmp = x_num.flat[i]
        x_num.flat[i] = tmp + h
        fxh1 = f(x_num)
        x_num.flat[i] = tmp - h
        fxh2 = f(x_num)
        dx_num.flat[i] = (fxh1 - fxh2) / (2 * h)
        x_num.flat[i] = tmp
        
    assert relative_error(x, dx_num, dx) < 1e-7
```

#### 4. 示例应用

##### 4.1 MNIST 分类器 (examples/mnist_mlp. py)

```python
import numpy as np
from core.layers import Linear
from core.activations import ReLU
from core.losses import CrossEntropyLoss

class MLP:
    def __init__(self):
        self.layers = [
            Linear(784, 100),
            ReLU(),
            Linear(100, 10)
        ]
        self.loss_fn = CrossEntropyLoss()
        
    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x
        
    def backward(self, dout=1):
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
            
    def update(self, lr=0.01):
        for layer in self.layers:
            if hasattr(layer, 'params'):
                for key in layer.params:
                    layer.params[key] -= lr * layer.grads[key]

def train_mnist():
    # 加载MNIST数据集
    from keras.datasets import mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    
    # 数据预处理
    x_train = x_train.reshape(-1, 784) / 255.0
    x_test = x_test.reshape(-1, 784) / 255.0
    y_train = np.eye(10)[y_train]
    y_test = np.eye(10)[y_test]
    
    # 训练模型
    model = MLP()
    batch_size = 100
    for epoch in range(5):
        for i in range(0, len(x_train), batch_size):
            batch_x = x_train[i:i+batch_size]
            batch_y = y_train[i:i+batch_size]
            
            # 前向传播
            out = model.forward(batch_x)
            loss = model.loss_fn.forward(out, batch_y)
            
            # 反向传播
            dout = model.loss_fn.backward()
            model.backward(dout)
            
            # 更新参数
            model.update(lr=0.1)
            
            if i % 1000 == 0:
                print(f"Epoch {epoch}, Step {i}, Loss: {loss:.4f}")
```

### 三、参考资源

#### 1. 官方文档

- [NumPy Documentation](https://numpy.org/doc/stable/)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [CS231n Course Notes](https://cs231n.github.io/)

#### 2. 教学视频

- [CS231n Lecture Videos](https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk)
- [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

#### 3. 推荐书籍

- Deep Learning Book (Goodfellow et al.)
  - [在线阅读](https://www.deeplearningbook.org/)
  - 重点章节：第 2-4 章（数学基础）、第 6 章（深度前馈网络）

#### 4. 开源实现参考

- [pytorch/pytorch](https://github.com/pytorch/pytorch)
- [keras-team/keras](https://github.com/keras-team/keras)

# 现代 CNN 架构实现指南 - Part 1：基础架构

## 一、项目结构

```
cnn_project/
├── models/
│   ├── __init__.py
│   ├── layers/
│   │   ├── __init__.py
│   │   ├── conv.py
│   │   ├── pool.py
│   │   ├── normalization.py
│   │   └── activation.py
│   ├── blocks/
│   │   ├── __init__.py
│   │   ├── basic_block.py
│   │   ├── bottleneck.py
│   │   └── attention.py
│   └── backbones/
│       ├── __init__.py
│       ├── resnet.py
│       └── efficientnet.py
├── configs/
│   ├── __init__.py
│   ├── default.py
│   └── experiment_configs/
│       ├── resnet50_cifar10.yaml
│       └── efficientnet_b0_cifar100.yaml
├── utils/
│   ├── __init__.py
│   ├── registry.py
│   ├── logger.py
│   └── visualization.py
├── data/
│   ├── __init__.py
│   ├── datasets.py
│   ├── transforms.py
│   └── loader.py
├── tools/
│   ├── train.py
│   ├── test.py
│   └── profile.py
├── tests/
│   ├── test_layers.py
│   ├── test_blocks.py
│   └── test_models.py
└── requirements.txt
```

## 二、配置系统设计

### 1. 基础配置类 (configs/default. py)

```python
from dataclasses import dataclass
from typing import List, Optional, Union

@dataclass
class ModelConfig:
    name: str
    in_channels: int = 3
    num_classes: int = 1000
    base_channels: int = 64
    layers: List[int] = (3, 4, 6, 3)  # ResNet-50
    dropout_rate: float = 0.0
    
@dataclass
class DataConfig:
    dataset: str
    data_dir: str
    batch_size: int = 128
    num_workers: int = 4
    pin_memory: bool = True
    
@dataclass
class TrainingConfig:
    epochs: int = 100
    optimizer: str = 'adam'
    learning_rate: float = 0.001
    weight_decay: float = 0.0001
    lr_scheduler: str = 'cosine'
    warmup_epochs: int = 5
    
@dataclass
class ExperimentConfig:
    exp_name: str
    model: ModelConfig
    data: DataConfig
    training: TrainingConfig
    seed: int = 42
```

### 2. YAML 配置示例 (configs/experiment_configs/resnet50_cifar10. yaml)

```yaml
exp_name: resnet50_cifar10_baseline

model:
  name: resnet50
  in_channels: 3
  num_classes: 10
  base_channels: 64
  layers: [3, 4, 6, 3]
  dropout_rate: 0.1

data:
  dataset: cifar10
  data_dir: data/cifar10
  batch_size: 128
  num_workers: 4
  pin_memory: true

training:
  epochs: 200
  optimizer: adamw
  learning_rate: 0.001
  weight_decay: 0.05
  lr_scheduler: cosine
  warmup_epochs: 5

seed: 42
```

## 三、CNN 基础组件实现

### 1. 卷积层 (models/layers/conv. py)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Union, Tuple

class Conv2dAuto(nn.Conv2d):
    """自动padding的卷积层"""
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[int, Tuple[int, int]],
        stride: Union[int, Tuple[int, int]] = 1,
        groups: int = 1,
        bias: bool = True
    ):
        if isinstance(kernel_size, int):
            kernel_size = (kernel_size,) * 2
        if isinstance(stride, int):
            stride = (stride,) * 2
            
        # 计算padding以保持特征图大小
        padding = tuple(k // 2 for k in kernel_size)
        super().__init__(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding, groups=groups, bias=bias
        )

class SeparableConv2d(nn.Module):
    """深度可分离卷积"""
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[int, Tuple[int, int]],
        stride: Union[int, Tuple[int, int]] = 1,
        padding: Union[int, Tuple[int, int]] = 0,
    ):
        super().__init__()
        
        self.depthwise = nn.Conv2d(
            in_channels, in_channels, kernel_size,
            stride=stride, padding=padding, groups=in_channels
        )
        self.pointwise = nn.Conv2d(
            in_channels, out_channels, kernel_size=1
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x
```

### 2. 池化层 (models/layers/pool. py)

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class AdaptiveConcatPool2d(nn.Module):
    """Concat pooling: 同时使用平均池化和最大池化"""
    def __init__(self, output_size: int = 1):
        super().__init__()
        self.ap = nn.AdaptiveAvgPool2d(output_size)
        self.mp = nn.AdaptiveMaxPool2d(output_size)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.cat([self.ap(x), self.mp(x)], dim=1)

class PyramidPooling(nn.Module):
    """金字塔池化模块"""
    def __init__(self, in_channels: int, levels: tuple = (1, 2, 3, 6)):
        super ().__init__()
        self. levels = levels
        self. paths = nn.ModuleList ([
            nn.Sequential (
                nn.AdaptiveAvgPool2d (level),
                nn.Conv2d (in_channels, in_channels // len (levels), 1),
                nn.BatchNorm2d (in_channels // len (levels)),
                nn.ReLU (inplace=True)
            ) for level in levels
        ])
        
    def forward (self, x: torch. Tensor) -> torch. Tensor:
        h, w = x.size (2), x.size (3)
        outputs = [x]
        
        for path in self. paths:
            pooled = path (x)
            upsampled = F.interpolate (
                pooled, size=(h, w), 
                mode='bilinear', 
                align_corners=False
            )
            outputs.append (upsampled)
            
        return torch.cat (outputs, dim=1)
```

### 3. 归一化层 (models/layers/normalization. py)

```python
import torch
import torch. nn as nn

class GroupNorm1 (nn. GroupNorm):
    """每通道分组归一化"""
    def __init__(self, num_channels: int, eps: float = 1e-5):
        super ().__init__(num_groups=1, num_channels=num_channels, eps=eps)

class CrossModalNorm (nn. Module):
    """跨模态归一化层"""
    def __init__(self, num_channels: int, num_groups: int = 32):
        super ().__init__()
        self. norm = nn.GroupNorm (num_groups, num_channels)
        self. modal_scale = nn.Parameter (torch.ones (1, num_channels, 1, 1))
        self. modal_bias = nn.Parameter (torch.zeros (1, num_channels, 1, 1))
        
    def forward (self, x: torch. Tensor) -> torch. Tensor:
        normalized = self.norm (x)
        return normalized * self. modal_scale + self. modal_bias
```

### 4. 实用工具函数 (utils/registry. py)

```python
from typing import Dict, Any
from collections import defaultdict

class Registry:
    """模块注册器，用于动态加载模型组件"""
    def __init__(self, name: str):
        self._name = name
        self._module_dict: Dict[str, Any] = dict ()
        
    def register_module (self, name: str = None):
        def _register (cls):
            key = name or cls.__name__
            if key in self._module_dict:
                raise ValueError (f"{key} is already registered in {self._name}")
            self._module_dict[key] = cls
            return cls
        return _register
    
    def get (self, name: str):
        if name not in self._module_dict:
            raise ValueError (f"{name} is not registered in {self._name}")
        return self._module_dict[name]

# 创建全局注册器
MODELS = Registry ('model')
DATASETS = Registry ('dataset')
TRANSFORMS = Registry ('transform')

# 使用示例
@MODELS. register_module ()
class ResNet (nn. Module):
    pass

@DATASETS. register_module ()
class CIFAR10Dataset:
    pass
```

## 四、使用说明

1. 首先安装依赖：

```bash
pip install -r requirements. txt
```

2. 创建配置文件：

```bash
# 在 configs/experiment_configs/下创建你的配置文件
cp configs/experiment_configs/resnet50_cifar10. yaml configs/experiment_configs/my_experiment. yaml
```

3. 修改配置文件：

```yaml
# 根据需要修改配置参数
exp_name: my_experiment
model:
  name: resnet50
  # ... 其他参数
```

4. 运行训练：

```bash
python tools/train. py --config configs/experiment_configs/my_experiment. yaml
```

## 五、参考资源

1. 论文：
- ResNet: Deep Residual Learning for Image Recognition
- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
- Group Normalization

2. 代码库：
- torchvision: https://github.com/pytorch/vision
- timm: https://github.com/rwightman/pytorch-image-models

3. 教程：
- PyTorch 官方教程：https://pytorch.org/tutorials/
- 动手深度学习：https://d2l.ai/

好的，这是 CNN 项目指南的第二部分：

# 现代 CNN 架构实现指南 - Part 2：模型架构实现

## 一、基础模块实现

### 1. 基础残差块 (models/blocks/basic_block.py)

```python
import torch
import torch.nn as nn
from typing import Optional

class BasicBlock(nn.Module):
    """基础残差块"""
    expansion = 1  # 输出通道数的扩张率

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        stride: int = 1,
        downsample: Optional[nn.Module] = None,
        groups: int = 1,
        base_width: int = 64,
        dropout_rate: float = 0.0
    ):
        super().__init__()
        if groups != 1 or base_width != 64:
            raise ValueError('BasicBlock只支持groups=1和base_width=64')
            
        # 第一个卷积层
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, kernel_size=3,
            stride=stride, padding=1, bias=False
        )
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # 第二个卷积层
        self.conv2 = nn.Conv2d(
            out_channels, out_channels, kernel_size=3,
            stride=1, padding=1, bias=False
        )
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.dropout = nn.Dropout(p=dropout_rate) if dropout_rate > 0 else None
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        identity = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        if self.dropout is not None:
            out = self.dropout(out)
            
        out = self.conv2(out)
        out = self.bn2(out)
        
        if self.downsample is not None:
            identity = self.downsample(x)
            
        out += identity
        out = self.relu(out)
        
        return out
```

### 2. Bottleneck块 (models/blocks/bottleneck.py)

```python
class Bottleneck(nn.Module):
    """带瓶颈的残差块"""
    expansion = 4  # 输出通道数的扩张率

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        stride: int = 1,
        downsample: Optional[nn.Module] = None,
        groups: int = 1,
        base_width: int = 64,
        dropout_rate: float = 0.0
    ):
        super().__init__()
        width = int(out_channels * (base_width / 64.)) * groups
        
        # 1x1 降维卷积
        self.conv1 = nn.Conv2d(
            in_channels, width, kernel_size=1, bias=False
        )
        self.bn1 = nn.BatchNorm2d(width)
        
        # 3x3 卷积
        self.conv2 = nn.Conv2d(
            width, width, kernel_size=3, stride=stride,
            padding=1, groups=groups, bias=False
        )
        self.bn2 = nn.BatchNorm2d(width)
        
        # 1x1 升维卷积
        self.conv3 = nn.Conv2d(
            width, out_channels * self.expansion,
            kernel_size=1, bias=False
        )
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.dropout = nn.Dropout(p=dropout_rate) if dropout_rate > 0 else None
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        identity = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        if self.dropout is not None:
            out = self.dropout(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        
        if self.dropout is not None:
            out = self.dropout(out)
            
        out = self.conv3(out)
        out = self.bn3(out)
        
        if self.downsample is not None:
            identity = self.downsample(x)
            
        out += identity
        out = self.relu(out)
        
        return out
```

### 3. 注意力模块 (models/blocks/attention.py)

```python
class SEBlock(nn.Module):
    """Squeeze-and-Excitation注意力块"""
    def __init__(self, channels: int, reduction: int = 16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class CBAM(nn.Module):
    """Convolutional Block Attention Module"""
    def __init__(self, channels: int, reduction: int = 16):
        super().__init__()
        
        # 通道注意力
        self.channel_gate = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels // reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1),
            nn.Sigmoid()
        )
        
        # 空间注意力
        self.spatial_gate = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # 通道注意力
        channel_att = self.channel_gate(x)
        x = x * channel_att
        
        # 空间注意力
        avg_pool = torch.mean(x, dim=1, keepdim=True)
        max_pool, _ = torch.max(x, dim=1, keepdim=True)
        spatial = torch.cat([avg_pool, max_pool], dim=1)
        spatial_att = self.spatial_gate(spatial)
        
        return x * spatial_att
```

## 二、完整模型架构

### 1. ResNet实现 (models/backbones/resnet.py)

```python
from typing import Type, List, Optional
from torch import nn
from ..blocks import BasicBlock, Bottleneck

@MODELS.register_module()
class ResNet(nn.Module):
    def __init__(
        self,
        block: Type[Union[BasicBlock, Bottleneck]],
        layers: List[int],
        num_classes: int = 1000,
        zero_init_residual: bool = False,
        groups: int = 1,
        width_per_group: int = 64,
        dropout_rate: float = 0.0,
        attention_type: Optional[str] = None
    ):
        super().__init__()
        self._norm_layer = nn.BatchNorm2d
        self.inplanes = 64
        self.base_width = width_per_group
        self.groups = groups
        
        # stem层
        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = self._norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # 残差层
        self.layer1 = self._make_layer(block, 64, layers[0], dropout_rate=dropout_rate)
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dropout_rate=dropout_rate)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dropout_rate=dropout_rate)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dropout_rate=dropout_rate)
        
        # 分类头
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
        
        # 注意力模块
        if attention_type == 'se':
            self.attention = SEBlock
        elif attention_type == 'cbam':
            self.attention = CBAM
        else:
            self.attention = None
            
        self._initialize_weights(zero_init_residual)
        
    def _make_layer(
        self,
        block: Type[Union[BasicBlock, Bottleneck]],
        planes: int,
        blocks: int,
        stride: int = 1,
        dropout_rate: float = 0.0
    ) -> nn.Sequential:
        norm_layer = self._norm_layer
        downsample = None
        
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.inplanes, planes * block.expansion,
                    kernel_size=1, stride=stride, bias=False
                ),
                norm_layer(planes * block.expansion),
            )
            
        layers = []
        layers.append(
            block(
                self.inplanes, planes, stride, downsample,
                self.groups, self.base_width, dropout_rate
            )
        )
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(
                block(
                    self.inplanes, planes,
                    groups=self.groups,
                    base_width=self.base_width,
                    dropout_rate=dropout_rate
                )
            )
            
        return nn.Sequential(*layers)
    
    def _initialize_weights(self, zero_init_residual: bool = False) -> None:
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)
                    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x

# 预定义的模型配置
def resnet50(**kwargs):
    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)

def resnet101(**kwargs):
    return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)
```

# 现代 CNN 架构实现指南 - Part 3：训练与实验管理

## 一、训练器实现 (tools/trainer.py)

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from typing import Dict, Any, Optional
import wandb
from tqdm import tqdm
from pathlib import Path

class Trainer:
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        criterion: nn.Module,
        optimizer: torch.optim.Optimizer,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        device: str = 'cuda',
        config: Dict[str, Any] = None,
        output_dir: str = 'outputs'
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.config = config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # 训练状态
        self.epoch = 0
        self.best_acc = 0.0
        self.best_model_path = None
        
        # 初始化wandb
        if config.get('use_wandb', False):
            wandb.init(
                project=config.get('project_name', 'cnn_training'),
                config=config
            )
            
    def train_epoch(self) -> Dict[str, float]:
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {self.epoch}')
        for batch_idx, (inputs, targets) in enumerate(pbar):
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            pbar.set_postfix({
                'loss': total_loss / (batch_idx + 1),
                'acc': 100. * correct / total
            })
            
        return {
            'train_loss': total_loss / len(self.train_loader),
            'train_acc': 100. * correct / total
        }
        
    @torch.no_grad()
    def validate(self) -> Dict[str, float]:
        """验证模型性能"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        for inputs, targets in tqdm(self.val_loader, desc='Validating'):
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
        return {
            'val_loss': total_loss / len(self.val_loader),
            'val_acc': 100. * correct / total
        }
        
    def save_checkpoint(self, metrics: Dict[str, float], is_best: bool = False):
        """保存检查点"""
        state = {
            'epoch': self.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_acc': self.best_acc,
            'metrics': metrics
        }
        
        # 保存最新检查点
        checkpoint_path = self.output_dir / f'checkpoint_epoch_{self.epoch}.pth'
        torch.save(state, checkpoint_path)
        
        # 如果是最佳模型，同时保存一份
        if is_best:
            best_path = self.output_dir / 'model_best.pth'
            torch.save(state, best_path)
            self.best_model_path = best_path
            
    def train(self, num_epochs: int):
        """完整训练流程"""
        for epoch in range(num_epochs):
            self.epoch = epoch
            
            # 训练一个epoch
            train_metrics = self.train_epoch()
            
            # 验证
            val_metrics = self.validate()
            
            # 更新学习率
            if self.scheduler is not None:
                self.scheduler.step()
            
            # 合并指标
            metrics = {**train_metrics, **val_metrics}
            
            # 记录到wandb
            if self.config.get('use_wandb', False):
                wandb.log(metrics, step=epoch)
                
            # 保存检查点
            is_best = val_metrics['val_acc'] > self.best_acc
            if is_best:
                self.best_acc = val_metrics['val_acc']
            self.save_checkpoint(metrics, is_best)
            
            # 打印指标
            print(f'Epoch {epoch} metrics:')
            for k, v in metrics.items():
                print(f'{k}: {v:.4f}')
```

## 二、实验配置和运行 (tools/train.py)

```python
import argparse
import yaml
import torch
from torch.utils.data import DataLoader
from utils.config import parse_config
from utils.build import build_model, build_dataset, build_optimizer, build_scheduler

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, required=True, help='配置文件路径')
    parser.add_argument('--local_rank', type=int, default=0, help='分布式训练的本地rank')
    return parser.parse_args()

def main():
    args = parse_args()
    
    # 加载配置
    with open(args.config) as f:
        config = yaml.safe_load(f)
    config = parse_config(config)
    
    # 设置随机种子
    torch.manual_seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(config.seed)
        
    # 构建数据集和加载器
    train_dataset = build_dataset(config.data, is_train=True)
    val_dataset = build_dataset(config.data, is_train=False)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.data.batch_size,
        shuffle=True,
        num_workers=config.data.num_workers,
        pin_memory=config.data.pin_memory
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.data.batch_size,
        shuffle=False,
        num_workers=config.data.num_workers,
        pin_memory=config.data.pin_memory
    )
    
    # 构建模型
    model = build_model(config.model)
    
    # 构建优化器
    optimizer = build_optimizer(config.training, model)
    
    # 构建学习率调度器
    scheduler = build_scheduler(config.training, optimizer)
    
    # 构建训练器
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=torch.nn.CrossEntropyLoss(),
        optimizer=optimizer,
        scheduler=scheduler,
        config=config,
        output_dir=f'outputs/{config.exp_name}'
    )
    
    # 开始训练
    trainer.train(config.training.epochs)

if __name__ == '__main__':
    main()
```

## 三、实验结果分析 (tools/analyze.py)

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import torch
import wandb
from typing import List, Dict

class ExperimentAnalyzer:
    """实验结果分析工具"""
    
    def __init__(self, exp_dir: str):
        self.exp_dir = Path(exp_dir)
        self.metrics_df = self._load_metrics()
        
    def _load_metrics(self) -> pd.DataFrame:
        """加载所有检查点的指标"""
        metrics_list = []
        for ckpt_path in self.exp_dir.glob('checkpoint_epoch_*.pth'):
            ckpt = torch.load(ckpt_path, map_location='cpu')
            metrics = ckpt['metrics']
            metrics['epoch'] = ckpt['epoch']
            metrics_list.append(metrics)
        return pd.DataFrame(metrics_list)
        
    def plot_learning_curves(self, save_path: str = None):
        """绘制学习曲线"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # 损失曲线
        ax1.plot(self.metrics_df['epoch'], self.metrics_df['train_loss'], label='Train')
        ax1.plot(self.metrics_df['epoch'], self.metrics_df['val_loss'], label='Val')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        
        # 准确率曲线
        ax2.plot(self.metrics_df['epoch'], self.metrics_df['train_acc'], label='Train')
        ax2.plot(self.metrics_df['epoch'], self.metrics_df['val_acc'], label='Val')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        
        if save_path:
            plt.savefig(save_path)
        plt.show()
        
    def compare_experiments(self, exp_ids: List[str], metric: str = 'val_acc'):
        """比较多个实验的结果"""
        api = wandb.Api()
        runs = [api.run(f"your-username/your-project/{exp_id}") for exp_id in exp_ids]
        
        plt.figure(figsize=(10, 6))
        for run in runs:
            history = pd.DataFrame(run.scan_history())
            plt.plot(history['_step'], history[metric], label=run.name)
            
        plt.xlabel('Step')
        plt.ylabel(metric)
        plt.legend()
        plt.show()
        
    def analyze_model(self, checkpoint_path: str):
        """分析模型参数统计信息"""
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        state_dict = ckpt['model_state_dict']
        
        stats = []
        for name, param in state_dict.items():
            param_numpy = param.numpy()
            stats.append({
                'name': name,
                'shape': list(param.shape),
                'mean': float(param_numpy.mean()),
                'std': float(param_numpy.std()),
                'min': float(param_numpy.min()),
                'max': float(param_numpy.max())
            })
        
        return pd.DataFrame(stats)
```

## 四、性能分析工具 (tools/profile.py)

```python
import torch
from torch.profiler import profile, record_function, ProfilerActivity
import time
from typing import List, Dict

class ModelProfiler:
    """模型性能分析器"""
    
    def __init__(self, model: torch.nn.Module, input_size: tuple):
        self.model = model
        self.input_size = input_size
        
    def profile_memory(self, batch_size: int = 1) -> Dict[str, float]:
        """分析模型内存使用"""
        input_tensor = torch.randn(batch_size, *self.input_size)
        torch.cuda.reset_peak_memory_stats()
        
        # 预热
        for _ in range(3):
            self.model(input_tensor.cuda())
            
        torch.cuda.synchronize()
        start_memory = torch.cuda.memory_allocated()
        
        # 运行推理
        output = self.model(input_tensor.cuda())
        torch.cuda.synchronize()
        
        peak_memory = torch.cuda.max_memory_allocated()
        end_memory = torch.cuda.memory_allocated()
        
        return {
            'start_memory_mb': start_memory / 1024**2,
            'peak_memory_mb': peak_memory / 1024**2,
            'end_memory_mb': end_memory / 1024**2
        }
        
    def profile_time(self, batch_size: int = 1, num_iterations: int = 100) -> Dict[str, float]:
        """分析模型推理时间"""
        input_tensor = torch.randn(batch_size, *self.input_size).cuda()
        self.model.cuda().eval()
        
        # 预热
        for _ in range(10):
            self.model(input_tensor)
            
        # 计时
        torch.cuda.synchronize()
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(num_iterations):
                self.model(input_tensor)
                
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_iterations
        return {
            'average_inference_time': avg_time,
            'fps': batch_size / avg_time
        }
        
    def profile_detailed(self, batch_size: int = 1):
        """详细的性能分析"""
        input_tensor = torch.randn(batch_size, *self.input_size).cuda()
        self.model.cuda().eval()
        
        with profile(
            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
            record_shapes=True,
            profile_memory=True
        ) as prof:
            with record_function("model_inference"):
                self.model(input_tensor)
                
        print(prof.key_averages().table(
            sort_by="cuda_time_total", row_limit=10))
        
        # 生成火焰图
        prof.export_chrome_trace("trace.json")
```

## 五、使用示例

1. 训练模型：

```bash
python tools/train.py --config configs/experiment_configs/resnet50_cifar10.yaml
```

2. 分析实验结果：

```python
from tools.analyze import ExperimentAnalyzer

analyzer = ExperimentAnalyzer('outputs/resnet50_cifar10')
analyzer.plot_learning_curves()

# 比较不同实验
analyzer.compare_experiments(['exp1', 'exp2', 'exp3'])
```

3. 性能分析：

```python
from tools.profile import ModelProfiler

profiler = ModelProfiler(model, input_size=(3, 224, 224))
memory_stats = profiler.profile_memory(batch_size=32)
time_stats = profiler
```

# 现代 CNN 架构实现指南 - Part 4：可视化与调试工具

## 一、特征图可视化 (utils/visualization.py)

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision.utils import make_grid
import numpy as np
from typing import List, Dict, Optional

class FeatureVisualizer:
    """特征图可视化工具"""
    def __init__(self, model: nn.Module):
        self.model = model
        self.hooks = []
        self.features = {}
        
    def _get_feature_hook(self, name: str):
        """返回特征图钩子函数"""
        def hook(module, input, output):
            self.features[name] = output.detach()
        return hook
        
    def register_hooks(self, layers_to_hook: Dict[str, nn.Module]):
        """注册钩子到指定层"""
        for name, layer in layers_to_hook.items():
            hook = layer.register_forward_hook(self._get_feature_hook(name))
            self.hooks.append(hook)
            
    def remove_hooks(self):
        """移除所有钩子"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
        
    def visualize_feature_maps(
        self,
        input_tensor: torch.Tensor,
        layer_name: str,
        num_channels: Optional[int] = None,
        figsize: tuple = (15, 15)
    ):
        """可视化特征图"""
        # 前向传播获取特征
        self.model.eval()
        with torch.no_grad():
            _ = self.model(input_tensor)
            
        # 获取特征图
        feature = self.features[layer_name]
        feature = feature[0]  # 只取第一个样本
        
        # 如果指定了通道数，只显示部分通道
        if num_channels is not None:
            feature = feature[:num_channels]
            
        # 创建网格图
        grid = make_grid(feature.unsqueeze(1), nrow=8, normalize=True, padding=2)
        
        plt.figure(figsize=figsize)
        plt.imshow(grid.cpu().numpy().transpose(1, 2, 0))
        plt.axis('off')
        plt.title(f'Feature Maps of {layer_name}')
        plt.show()
        
    def visualize_channel_attention(
        self,
        input_tensor: torch.Tensor,
        layer_name: str,
        figsize: tuple = (10, 6)
    ):
        """可视化通道注意力权重"""
        self.model.eval()
        with torch.no_grad():
            _ = self.model(input_tensor)
            
        feature = self.features[layer_name]
        attention_weights = torch.mean(feature, dim=(2, 3))
        
        plt.figure(figsize=figsize)
        sns.heatmap(
            attention_weights.cpu().numpy(),
            cmap='viridis',
            cbar_kws={'label': 'Attention Weight'}
        )
        plt.title(f'Channel Attention Weights of {layer_name}')
        plt.xlabel('Channel Index')
        plt.ylabel('Sample Index')
        plt.show()
```

## 二、模型调试工具 (utils/debug.py)

```python
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Union, Tuple

class ModelDebugger:
    """模型调试工具"""
    def __init__(self, model: nn.Module):
        self.model = model
        self.grad_stats = {}
        self.activation_stats = {}
        
    def _get_grad_hook(self, name: str):
        """梯度钩子"""
        def hook(grad):
            if name not in self.grad_stats:
                self.grad_stats[name] = []
            stats = {
                'mean': grad.mean().item(),
                'std': grad.std().item(),
                'norm': grad.norm().item(),
                'max': grad.max().item(),
                'min': grad.min().item()
            }
            self.grad_stats[name].append(stats)
        return hook
        
    def _get_activation_hook(self, name: str):
        """激活值钩子"""
        def hook(module, input, output):
            if name not in self.activation_stats:
                self.activation_stats[name] = []
            stats = {
                'mean': output.mean().item(),
                'std': output.std().item(),
                'norm': output.norm().item(),
                'max': output.max().item(),
                'min': output.min().item(),
                'zero_fraction': (output == 0).float().mean().item()
            }
            self.activation_stats[name].append(stats)
        return hook
        
    def register_hooks(self, layers_to_debug: Dict[str, nn.Module]):
        """注册调试钩子"""
        self.hooks = []
        for name, layer in layers_to_debug.items():
            # 注册梯度钩子
            if layer.weight is not None:
                hook = layer.weight.register_hook(self._get_grad_hook(f"{name}_weight"))
                self.hooks.append(hook)
            if layer.bias is not None:
                hook = layer.bias.register_hook(self._get_grad_hook(f"{name}_bias"))
                self.hooks.append(hook)
                
            # 注册激活值钩子
            hook = layer.register_forward_hook(self._get_activation_hook(name))
            self.hooks.append(hook)
            
    def remove_hooks(self):
        """移除所有钩子"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
        
    def check_nan_gradients(self) -> List[str]:
        """检查是否存在NaN梯度"""
        nan_layers = []
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    nan_layers.append(name)
        return nan_layers
        
    def check_vanishing_gradients(self, threshold: float = 1e-4) -> List[str]:
        """检查是否存在梯度消失"""
        vanishing_layers = []
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                if param.grad.abs().mean() < threshold:
                    vanishing_layers.append(name)
        return vanishing_layers
        
    def analyze_layer_statistics(self) -> Dict[str, Dict[str, float]]:
        """分析层统计信息"""
        layer_stats = {}
        for name, param in self.model.named_parameters():
            layer_stats[name] = {
                'weight_mean': param.data.mean().item(),
                'weight_std': param.data.std().item(),
                'weight_norm': param.data.norm().item()
            }
            if param.grad is not None:
                layer_stats[name].update({
                    'grad_mean': param.grad.mean().item(),
                    'grad_std': param.grad.std().item(),
                    'grad_norm': param.grad.norm().item()
                })
        return layer_stats
        
    def plot_gradient_flow(self, figsize: tuple = (10, 6)):
        """绘制梯度流"""
        plt.figure(figsize=figsize)
        
        named_parameters = list(self.model.named_parameters())
        max_y = 0
        min_y = 0
        
        for idx, (name, param) in enumerate(named_parameters):
            if param.grad is not None:
                grad_mean = param.grad.abs().mean().item()
                plt.semilogy(idx, grad_mean, 'o')
                max_y = max(max_y, grad_mean)
                min_y = min(min_y, grad_mean)
                
        plt.xticks(range(len(named_parameters)), 
                  [name for name, _ in named_parameters], 
                  rotation=45, ha='right')
        plt.xlabel('Layers')
        plt.ylabel('average gradient')
        plt.title('Gradient flow')
        plt.grid(True)
        plt.show()
```

## 三、性能监控工具 (utils/monitor.py)

```python
import torch
import psutil
import time
import threading
from typing import Dict, List
import matplotlib.pyplot as plt
from collections import deque

class PerformanceMonitor:
    """性能监控工具"""
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.gpu_memory = deque(maxlen=window_size)
        self.cpu_memory = deque(maxlen=window_size)
        self.gpu_utilization = deque(maxlen=window_size)
        self.cpu_utilization = deque(maxlen=window_size)
        self.monitoring = False
        
    def start_monitoring(self, interval: float = 1.0):
        """开始监控"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,)
        )
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """停止监控"""
        self.monitoring = False
        self.monitor_thread.join()
        
    def _monitor_loop(self, interval: float):
        """监控循环"""
        while self.monitoring:
            # GPU监控
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / 1024**2
                self.gpu_memory.append(memory_allocated)
                
                if hasattr(torch.cuda, 'utilization'):
                    gpu_util = torch.cuda.utilization()
                    self.gpu_utilization.append(gpu_util)
                    
            # CPU监控
            cpu_percent = psutil.cpu_percent()
            memory_percent = psutil.virtual_memory().percent
            
            self.cpu_utilization.append(cpu_percent)
            self.cpu_memory.append(memory_percent)
            
            time.sleep(interval)
            
    def plot_metrics(self, figsize: tuple = (12, 8)):
        """绘制监控指标"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)
        
        # GPU内存
        ax1.plot(list(self.gpu_memory))
        ax1.set_title('GPU Memory Usage (MB)')
        ax1.set_xlabel('Time')
        ax1.grid(True)
        
        # GPU利用率
        ax2.plot(list(self.gpu_utilization))
        ax2.set_title('GPU Utilization (%)')
        ax2.set_xlabel('Time')
        ax2.grid(True)
        
        # CPU内存
        ax3.plot(list(self.cpu_memory))
        ax3.set_title('CPU Memory Usage (%)')
        ax3.set_xlabel('Time')
        ax3.grid(True)
        
        # CPU利用率
        ax4.plot(list(self.cpu_utilization))
        ax4.set_title('CPU Utilization (%)')
        ax4.set_xlabel('Time')
        ax4.grid(True)
        
        plt.tight_layout()
        plt.show()
        
    def get_current_stats(self) -> Dict[str, float]:
        """获取当前统计信息"""
        return {
            'gpu_memory_mb': self.gpu_memory[-1] if self.gpu_memory else 0,
            'gpu_util_percent': self.gpu_utilization[-1] if self.gpu_utilization else 0,
            'cpu_memory_percent': self.cpu_memory[-1] if self.cpu_memory else 0,
            'cpu_util_percent': self.cpu_utilization[-1] if self.cpu_utilization else 0
        }
```

## 四、使用示例

### 1. 特征图可视化

```python
# 初始化可视化器
visualizer = FeatureVisualizer(model)

# 注册要可视化的层
layers_to_viz = {
    'conv1': model.conv1,
    'layer1': model.layer1[0].conv1,
    'layer2': model.layer2[0].conv1
}
visualizer.register_hooks(layers_to_viz)

# 可视化特征图
input_tensor = torch.randn(1, 3, 224, 224).cuda()
visualizer.visualize_feature_maps(input_tensor, 'conv1', num_channels=16)

# 可视化注意力权重
visualizer.visualize_channel_attention(input_tensor, 'layer1')
```

### 2. 模型调试

```python
# 初始化调试器
debugger = ModelDebugger(model)

# 注册要调试的层
layers_to_debug = {
    'conv1': model.conv1,
    'bn1': model.bn1,
    'layer1.0': model.layer1[0]
}
debugger.register_hooks(layers_to_debug)

# 训练时检查问题
for batch in train_loader:
    outputs = model(batch)
    loss = criterion(outputs, targets)
    loss.backward()
    
    # 检查梯度问题
    nan_layers = debugger.check_nan_gradients()
    vanishing_layers = debugger.check_vanishing_gradients()
    
    if nan_layers or vanishing_layers:
        print("Found problematic layers:", nan_layers, vanishing_layers)
        
    # 分析层统计信息
    stats = debugger.analyze_layer_statistics()
    
    optimizer.step()
```

### 3. 性能监控

```python
# 初始化监控器
monitor = PerformanceMonitor()

# 开始监控
monitor.start_monitoring(interval=0.5)

# 训练模型
try:
    for epoch in range(num_epochs):
        for batch in train_loader:
            # 训练步骤
            if epoch % 10 == 0:
                # 打印当前性能统计
                stats = monitor.get_current_stats()
                print(f"Performance stats: {stats}")
finally:
    # 停止监控
    monitor.stop_monitoring()
    
# 绘制监控指标
monitor.plot_metrics()
```