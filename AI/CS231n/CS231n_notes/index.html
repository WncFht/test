<!DOCTYPE html>
<html class="no-js" lang="zh">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="wnc" name="author"/>
<link href="https://WncFht.github.io/test/AI/CS231n/CS231n_notes/" rel="canonical"/>
<link href="../../%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" rel="prev"/>
<link href="../Image%20Classification-Data-driven%20Approach%2C%20k-Nearest%20Neighbor%2C%20train_val_test%20splits/" rel="next"/>
<link href="../../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.5.47" name="generator"/>
<title>Computer Vision - wnc 的咖啡馆</title>
<link href="../../../assets/stylesheets/main.6f8fc17f.min.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link href="../../../css/heti.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet"/>
<link href="https://cdn.tonycrane.cc/jbmono/jetbrainsmono.css" rel="stylesheet"/>
<link href="https://cdn.tonycrane.cc/lxgw/lxgwscreen.css" rel="stylesheet"/>
<link href="../../../css/custom.css" rel="stylesheet"/>
<link href="../../../css/tasklist.css" rel="stylesheet"/>
<link href="../../../css/card.css" rel="stylesheet"/>
<link href="../../../css/flink.css" rel="stylesheet"/>
<script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="indigo" data-md-color-primary="blue-grey" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#computer-vision">
          跳转至
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="页眉" class="md-header__inner md-grid">
<a aria-label="wnc 的咖啡馆" class="md-header__button md-logo" data-md-component="logo" href="../../.." title="wnc 的咖啡馆">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            wnc 的咖啡馆
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Computer Vision
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="切换至夜间模式" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="blue-grey" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="切换至夜间模式">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
<input aria-label="切换至日间模式" class="md-option" data-md-color-accent="indigo" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="切换至日间模式">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="搜索" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="搜索" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</label>
<nav aria-label="查找" class="md-search__options">
<button aria-label="清空当前内容" class="md-search__icon md-icon" tabindex="-1" title="清空当前内容" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/WncFht/test/" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"></path></svg>
</div>
<div class="md-source__repository">
    wnc's café
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="标签" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../..">
          
  
    
  
  Home

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../CS%20basic/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/">
          
  
    
  
  计算机基础

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../traditional%20cv/%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/">
          
  
    
  
  传统计算机视觉

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../../Dive%20into%20Deep%20Learning/">
          
  
    
  
  AI

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../Robot/pnp/">
          
  
    
  
  Robot

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../Environment/Ubuntu_setup/">
          
  
    
  
  环境配置

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../Technology/AI%20usage/">
          
  
    
  
  一些工具

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../summary/202409-10/">
          
  
    
  
  总结

        </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="导航栏" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="wnc 的咖啡馆" class="md-nav__button md-logo" data-md-component="logo" href="../../.." title="wnc 的咖啡馆">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"></path></svg>
</a>
    wnc 的咖啡馆
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/WncFht/test/" title="前往仓库">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"></path></svg>
</div>
<div class="md-source__repository">
    wnc's café
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_1" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../..">
<span class="md-ellipsis">
    Home
  </span>
</a>
<label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="false" aria-labelledby="__nav_1_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_1">
<span class="md-nav__icon md-icon"></span>
            Home
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../links/">
<span class="md-ellipsis">
    友链
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
<span class="md-ellipsis">
    计算机基础
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
            计算机基础
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/">
<span class="md-ellipsis">
    深入理解计算机系统
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E4%B8%8E%E8%AE%BE%E8%AE%A1%E7%A1%AC%E4%BB%B6%E8%BD%AF%E4%BB%B6%E6%8E%A5%E5%8F%A3/">
<span class="md-ellipsis">
    计算机组成与设计硬件软件接口
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
<span class="md-ellipsis">
    C++
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_3">
<span class="md-nav__icon md-icon"></span>
            C++
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/C%2B%2B/Accelerated%20C%2B%2B/">
<span class="md-ellipsis">
    Accelerated C++
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/C%2B%2B/C%2B%2B%E8%AF%AD%E6%B3%95/">
<span class="md-ellipsis">
    C++
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
<span class="md-ellipsis">
    cs61a
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_4">
<span class="md-nav__icon md-icon"></span>
            cs61a
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61a/COMPOSING%20PROGRAMS/">
<span class="md-ellipsis">
    COMPOSING PROGRAMS
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61a/cs61a/">
<span class="md-ellipsis">
    Cs61a
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
<span class="md-ellipsis">
    cs61c
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_5_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_5">
<span class="md-nav__icon md-icon"></span>
            cs61c
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c/">
<span class="md-ellipsis">
    Cs61c
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec06/">
<span class="md-ellipsis">
    Cs61c lec06
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec07/">
<span class="md-ellipsis">
    Cs61c lec07
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec08/">
<span class="md-ellipsis">
    Cs61c lec08
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec09/">
<span class="md-ellipsis">
    Cs61c lec09
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec10/">
<span class="md-ellipsis">
    Cs61c lec10
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec11/">
<span class="md-ellipsis">
    Cs61c lec11
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec12/">
<span class="md-ellipsis">
    Cs61c lec12
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec13/">
<span class="md-ellipsis">
    Cs61c lec13
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec14/">
<span class="md-ellipsis">
    Cs61c lec14
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/cs61c_lec15/">
<span class="md-ellipsis">
    Cs61c lec15
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/cs61c/%E8%B7%B3%E8%BD%AC%E5%92%8C%E8%BF%94%E5%9B%9E%E7%9A%84%E5%87%BD%E6%95%B0/">
<span class="md-ellipsis">
    跳转和返回的函数
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
<span class="md-ellipsis">
    计算机网络
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_6_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_6">
<span class="md-nav__icon md-icon"></span>
            计算机网络
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../CS%20basic/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E5%9F%BA%E7%A1%80/">
<span class="md-ellipsis">
    计算机网络安全基础
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
<span class="md-ellipsis">
    传统计算机视觉
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
            传统计算机视觉
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../traditional%20cv/%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/">
<span class="md-ellipsis">
    视觉SLAM十四讲
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/>
<div class="md-nav__link md-nav__container">
<a class="md-nav__link" href="../../">
<span class="md-ellipsis">
    AI
  </span>
</a>
<label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-expanded="true" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
            AI
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../Dive%20into%20Deep%20Learning/">
<span class="md-ellipsis">
    Dive into Deep Learning
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">
<span class="md-ellipsis">
    统计学习方法
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_4_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
<span class="md-ellipsis">
    CS231n
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_4_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_4_4">
<span class="md-nav__icon md-icon"></span>
            CS231n
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Computer Vision
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    Computer Vision
  </span>
</a>
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      目录
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#1-introduction">
<span class="md-ellipsis">
      1 - Introduction
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#2-image-classification">
<span class="md-ellipsis">
      2 - Image Classification
    </span>
</a>
<nav aria-label="2 - Image Classification" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#k-nearest-neighbor">
<span class="md-ellipsis">
      K Nearest Neighbor
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#linear-classifier">
<span class="md-ellipsis">
      Linear Classifier
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#3-loss-functions-and-optimization">
<span class="md-ellipsis">
      3 - Loss Functions and Optimization
    </span>
</a>
<nav aria-label="3 - Loss Functions and Optimization" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#loss-functions">
<span class="md-ellipsis">
      Loss Functions
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#optimization">
<span class="md-ellipsis">
      Optimization
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#4-neural-networks-and-backpropagation">
<span class="md-ellipsis">
      4 - Neural Networks and Backpropagation
    </span>
</a>
<nav aria-label="4 - Neural Networks and Backpropagation" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#neural-networks">
<span class="md-ellipsis">
      Neural Networks
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#backpropagation">
<span class="md-ellipsis">
      Backpropagation
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#5-convolutional-neural-networks">
<span class="md-ellipsis">
      5 - Convolutional Neural Networks
    </span>
</a>
<nav aria-label="5 - Convolutional Neural Networks" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#convolution-layer">
<span class="md-ellipsis">
      Convolution Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#pooling-layer">
<span class="md-ellipsis">
      Pooling layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#convolutional-neural-networks-cnn">
<span class="md-ellipsis">
      Convolutional Neural Networks (CNN)
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#6-cnn-architectures">
<span class="md-ellipsis">
      6 - CNN Architectures
    </span>
</a>
<nav aria-label="6 - CNN Architectures" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#alexnet">
<span class="md-ellipsis">
      AlexNet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#vgg">
<span class="md-ellipsis">
      VGG
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#googlenet">
<span class="md-ellipsis">
      GoogLeNet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#resnet">
<span class="md-ellipsis">
      ResNet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#senet">
<span class="md-ellipsis">
      SENet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#improvements-of-resnet">
<span class="md-ellipsis">
      Improvements of ResNet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#other-interesting-networks">
<span class="md-ellipsis">
      Other Interesting Networks
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#7-training-neural-networks">
<span class="md-ellipsis">
      7 - Training Neural Networks
    </span>
</a>
<nav aria-label="7 - Training Neural Networks" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#activation-functions">
<span class="md-ellipsis">
      Activation Functions
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#data-processing">
<span class="md-ellipsis">
      Data Processing
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#weight-initialization">
<span class="md-ellipsis">
      Weight Initialization
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#batch-normalization">
<span class="md-ellipsis">
      Batch Normalization
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#transfer-learning">
<span class="md-ellipsis">
      Transfer Learning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#regularization">
<span class="md-ellipsis">
      Regularization
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#hyperparameter-tuning">
<span class="md-ellipsis">
      Hyperparameter Tuning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#gradient-checks">
<span class="md-ellipsis">
      Gradient Checks
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#8-visualizing-and-understanding">
<span class="md-ellipsis">
      8 - Visualizing and Understanding
    </span>
</a>
<nav aria-label="8 - Visualizing and Understanding" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#feature-visualization-and-inversion">
<span class="md-ellipsis">
      Feature Visualization and Inversion
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adversarial-examples">
<span class="md-ellipsis">
      Adversarial Examples
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#deepdream-and-style-transfer">
<span class="md-ellipsis">
      DeepDream and Style Transfer
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#9-object-detection-and-image-segmentation">
<span class="md-ellipsis">
      9 - Object Detection and Image Segmentation
    </span>
</a>
<nav aria-label="9 - Object Detection and Image Segmentation" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#semantic-segmentation">
<span class="md-ellipsis">
      Semantic Segmentation
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#object-detection">
<span class="md-ellipsis">
      Object Detection
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#instance-segmentation">
<span class="md-ellipsis">
      Instance Segmentation
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#10-recurrent-neural-networks">
<span class="md-ellipsis">
      10 - Recurrent Neural Networks
    </span>
</a>
<nav aria-label="10 - Recurrent Neural Networks" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#recurrent-neural-network-rnn">
<span class="md-ellipsis">
      Recurrent Neural Network (RNN)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#long-short-term-memory-lstm">
<span class="md-ellipsis">
      Long Short Term Memory (LSTM)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#other-rnn-variants">
<span class="md-ellipsis">
      Other RNN Variants
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#11-attention-and-transformers">
<span class="md-ellipsis">
      11 - Attention and Transformers
    </span>
</a>
<nav aria-label="11 - Attention and Transformers" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#rnn-with-attention">
<span class="md-ellipsis">
      RNN with Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#general-attention-layer">
<span class="md-ellipsis">
      General Attention Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#self-attention-layer">
<span class="md-ellipsis">
      Self-attention Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#positional-encoding">
<span class="md-ellipsis">
      Positional Encoding
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#masked-self-attention-layer">
<span class="md-ellipsis">
      Masked Self-attention Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multi-head-self-attention-layer">
<span class="md-ellipsis">
      Multi-head Self-attention Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#transformer">
<span class="md-ellipsis">
      Transformer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#comparing-rnns-to-transformer">
<span class="md-ellipsis">
      Comparing RNNs to Transformer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#comparing-convnets-to-transformer">
<span class="md-ellipsis">
      Comparing ConvNets to Transformer
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#12-video-understanding">
<span class="md-ellipsis">
      12 - Video Understanding
    </span>
</a>
<nav aria-label="12 - Video Understanding" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#video-classification">
<span class="md-ellipsis">
      Video Classification
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#plain-cnn-structure">
<span class="md-ellipsis">
      Plain CNN Structure
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#modeling-long-term-temporal-structure">
<span class="md-ellipsis">
      Modeling Long-term Temporal Structure
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#visualizing-video-models">
<span class="md-ellipsis">
      Visualizing Video Models
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multimodal-video-understanding">
<span class="md-ellipsis">
      Multimodal Video Understanding
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#13-generative-models">
<span class="md-ellipsis">
      13 - Generative Models
    </span>
</a>
<nav aria-label="13 - Generative Models" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#pixelrnn-and-pixelcnn">
<span class="md-ellipsis">
      PixelRNN and PixelCNN
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#variational-autoencoder">
<span class="md-ellipsis">
      Variational Autoencoder
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#generative-adversarial-networks-gans">
<span class="md-ellipsis">
      Generative Adversarial Networks (GANs)
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#14-self-supervised-learning">
<span class="md-ellipsis">
      14 - Self-supervised Learning
    </span>
</a>
<nav aria-label="14 - Self-supervised Learning" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#pretext-tasks">
<span class="md-ellipsis">
      Pretext Tasks
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#contrastive-representation-learning">
<span class="md-ellipsis">
      Contrastive Representation Learning
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#15-low-level-vision">
<span class="md-ellipsis">
      15 - Low-Level Vision
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#16-3d-vision">
<span class="md-ellipsis">
      16 - 3D Vision
    </span>
</a>
<nav aria-label="16 - 3D Vision" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#representation">
<span class="md-ellipsis">
      Representation
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ai-3d">
<span class="md-ellipsis">
      AI + 3D
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../Image%20Classification-Data-driven%20Approach%2C%20k-Nearest%20Neighbor%2C%20train_val_test%20splits/">
<span class="md-ellipsis">
    Image Classification-Data-driven Approach, k-Nearest Neighbor, train_val_test splits
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../Linear%20classification-Support%20Vector%20Machine%2C%20Softmax/">
<span class="md-ellipsis">
    Linear classification-Support Vector Machine, Softmax
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../Python%20Numpy/">
<span class="md-ellipsis">
    Python Numpy
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
<span class="md-ellipsis">
    EECS 498-007
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_5_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_4_5">
<span class="md-nav__icon md-icon"></span>
            EECS 498-007
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../EECS%20498-007/KNN/">
<span class="md-ellipsis">
    KNN
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../EECS%20498-007/linear_classifer/">
<span class="md-ellipsis">
    Linear classifer
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../EECS%20498-007/pytorch%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/">
<span class="md-ellipsis">
    pytorch 的基本使用
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
<span class="md-ellipsis">
    ffb6d
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_6_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_4_6">
<span class="md-nav__icon md-icon"></span>
            ffb6d
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../ffb6d/ffb6d-docker/">
<span class="md-ellipsis">
    Docker从入门到实践：以FFB6D环境配置为例
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../ffb6d/ffb6d/">
<span class="md-ellipsis">
    FFB6D环境配置指南：原生系统安装
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
<span class="md-ellipsis">
    Robot
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_5_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
            Robot
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Robot/pnp/">
<span class="md-ellipsis">
    pnp
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Robot/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/">
<span class="md-ellipsis">
    卡尔曼滤波
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Robot/%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/">
<span class="md-ellipsis">
    相机标定
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
<span class="md-ellipsis">
    环境配置
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_6_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_6">
<span class="md-nav__icon md-icon"></span>
            环境配置
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Environment/Ubuntu_setup/">
<span class="md-ellipsis">
    Ubuntu 配置
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Environment/obsidian_setup/">
<span class="md-ellipsis">
    obsidian 配置
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Environment/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">
<span class="md-ellipsis">
    开发环境配置
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_7" type="checkbox"/>
<label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
<span class="md-ellipsis">
    一些工具
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_7_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_7">
<span class="md-nav__icon md-icon"></span>
            一些工具
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Technology/AI%20usage/">
<span class="md-ellipsis">
    AI 使用
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Technology/CMake/">
<span class="md-ellipsis">
    CMake 相关
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Technology/Makeflie/">
<span class="md-ellipsis">
    Makeflie
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Technology/SSH/">
<span class="md-ellipsis">
    SSH配置指南
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Technology/Tabby%20%2B%20Zsh/">
<span class="md-ellipsis">
    Tabby + Zsh 配置指南
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Technology/chezmoi/">
<span class="md-ellipsis">
    用 chezmoi 实现跨设备同步配置
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Technology/mkdocs%20material/">
<span class="md-ellipsis">
    mkdocs material 超全配置
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../Technology/prompt/">
<span class="md-ellipsis">
    常用 prompt 记录
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_8" type="checkbox"/>
<label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
<span class="md-ellipsis">
    总结
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_8_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_8">
<span class="md-nav__icon md-icon"></span>
            总结
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/202409-10/">
<span class="md-ellipsis">
    2024 年 9 月 10 月总结
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/20241028-1103/">
<span class="md-ellipsis">
    2024 年第 44 周周结
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/20241104-1110/">
<span class="md-ellipsis">
    2024 年第 45 周周结
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/20241111-1117/">
<span class="md-ellipsis">
    2024 年第 46 周周结
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/20241118-1124/">
<span class="md-ellipsis">
    2024 年第 47 周周结
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/20241125-1201/">
<span class="md-ellipsis">
    2024 年第 48 周周结
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/2024summer_vacation/">
<span class="md-ellipsis">
    2024年高三-大一暑假总结
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_8_8" type="checkbox"/>
<label class="md-nav__link" for="__nav_8_8" id="__nav_8_8_label" tabindex="0">
<span class="md-ellipsis">
    high school
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_8_8_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_8_8">
<span class="md-nav__icon md-icon"></span>
            high school
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/high%20school/eng_fht/">
<span class="md-ellipsis">
    英语经验分享
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/high%20school/math_fht/">
<span class="md-ellipsis">
    数学经验分享
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/high%20school/phy_fht/">
<span class="md-ellipsis">
    物理经验分享
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/high%20school/tech_fht/">
<span class="md-ellipsis">
    技术经验分享
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../summary/high%20school/%E4%B8%80%E4%BA%9B%E8%B5%84%E6%BA%90/">
<span class="md-ellipsis">
    一些资源
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="目录" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      目录
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#1-introduction">
<span class="md-ellipsis">
      1 - Introduction
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#2-image-classification">
<span class="md-ellipsis">
      2 - Image Classification
    </span>
</a>
<nav aria-label="2 - Image Classification" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#k-nearest-neighbor">
<span class="md-ellipsis">
      K Nearest Neighbor
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#linear-classifier">
<span class="md-ellipsis">
      Linear Classifier
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#3-loss-functions-and-optimization">
<span class="md-ellipsis">
      3 - Loss Functions and Optimization
    </span>
</a>
<nav aria-label="3 - Loss Functions and Optimization" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#loss-functions">
<span class="md-ellipsis">
      Loss Functions
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#optimization">
<span class="md-ellipsis">
      Optimization
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#4-neural-networks-and-backpropagation">
<span class="md-ellipsis">
      4 - Neural Networks and Backpropagation
    </span>
</a>
<nav aria-label="4 - Neural Networks and Backpropagation" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#neural-networks">
<span class="md-ellipsis">
      Neural Networks
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#backpropagation">
<span class="md-ellipsis">
      Backpropagation
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#5-convolutional-neural-networks">
<span class="md-ellipsis">
      5 - Convolutional Neural Networks
    </span>
</a>
<nav aria-label="5 - Convolutional Neural Networks" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#convolution-layer">
<span class="md-ellipsis">
      Convolution Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#pooling-layer">
<span class="md-ellipsis">
      Pooling layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#convolutional-neural-networks-cnn">
<span class="md-ellipsis">
      Convolutional Neural Networks (CNN)
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#6-cnn-architectures">
<span class="md-ellipsis">
      6 - CNN Architectures
    </span>
</a>
<nav aria-label="6 - CNN Architectures" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#alexnet">
<span class="md-ellipsis">
      AlexNet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#vgg">
<span class="md-ellipsis">
      VGG
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#googlenet">
<span class="md-ellipsis">
      GoogLeNet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#resnet">
<span class="md-ellipsis">
      ResNet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#senet">
<span class="md-ellipsis">
      SENet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#improvements-of-resnet">
<span class="md-ellipsis">
      Improvements of ResNet
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#other-interesting-networks">
<span class="md-ellipsis">
      Other Interesting Networks
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#7-training-neural-networks">
<span class="md-ellipsis">
      7 - Training Neural Networks
    </span>
</a>
<nav aria-label="7 - Training Neural Networks" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#activation-functions">
<span class="md-ellipsis">
      Activation Functions
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#data-processing">
<span class="md-ellipsis">
      Data Processing
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#weight-initialization">
<span class="md-ellipsis">
      Weight Initialization
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#batch-normalization">
<span class="md-ellipsis">
      Batch Normalization
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#transfer-learning">
<span class="md-ellipsis">
      Transfer Learning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#regularization">
<span class="md-ellipsis">
      Regularization
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#hyperparameter-tuning">
<span class="md-ellipsis">
      Hyperparameter Tuning
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#gradient-checks">
<span class="md-ellipsis">
      Gradient Checks
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#8-visualizing-and-understanding">
<span class="md-ellipsis">
      8 - Visualizing and Understanding
    </span>
</a>
<nav aria-label="8 - Visualizing and Understanding" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#feature-visualization-and-inversion">
<span class="md-ellipsis">
      Feature Visualization and Inversion
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#adversarial-examples">
<span class="md-ellipsis">
      Adversarial Examples
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#deepdream-and-style-transfer">
<span class="md-ellipsis">
      DeepDream and Style Transfer
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#9-object-detection-and-image-segmentation">
<span class="md-ellipsis">
      9 - Object Detection and Image Segmentation
    </span>
</a>
<nav aria-label="9 - Object Detection and Image Segmentation" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#semantic-segmentation">
<span class="md-ellipsis">
      Semantic Segmentation
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#object-detection">
<span class="md-ellipsis">
      Object Detection
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#instance-segmentation">
<span class="md-ellipsis">
      Instance Segmentation
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#10-recurrent-neural-networks">
<span class="md-ellipsis">
      10 - Recurrent Neural Networks
    </span>
</a>
<nav aria-label="10 - Recurrent Neural Networks" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#recurrent-neural-network-rnn">
<span class="md-ellipsis">
      Recurrent Neural Network (RNN)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#long-short-term-memory-lstm">
<span class="md-ellipsis">
      Long Short Term Memory (LSTM)
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#other-rnn-variants">
<span class="md-ellipsis">
      Other RNN Variants
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#11-attention-and-transformers">
<span class="md-ellipsis">
      11 - Attention and Transformers
    </span>
</a>
<nav aria-label="11 - Attention and Transformers" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#rnn-with-attention">
<span class="md-ellipsis">
      RNN with Attention
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#general-attention-layer">
<span class="md-ellipsis">
      General Attention Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#self-attention-layer">
<span class="md-ellipsis">
      Self-attention Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#positional-encoding">
<span class="md-ellipsis">
      Positional Encoding
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#masked-self-attention-layer">
<span class="md-ellipsis">
      Masked Self-attention Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multi-head-self-attention-layer">
<span class="md-ellipsis">
      Multi-head Self-attention Layer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#transformer">
<span class="md-ellipsis">
      Transformer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#comparing-rnns-to-transformer">
<span class="md-ellipsis">
      Comparing RNNs to Transformer
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#comparing-convnets-to-transformer">
<span class="md-ellipsis">
      Comparing ConvNets to Transformer
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#12-video-understanding">
<span class="md-ellipsis">
      12 - Video Understanding
    </span>
</a>
<nav aria-label="12 - Video Understanding" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#video-classification">
<span class="md-ellipsis">
      Video Classification
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#plain-cnn-structure">
<span class="md-ellipsis">
      Plain CNN Structure
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#modeling-long-term-temporal-structure">
<span class="md-ellipsis">
      Modeling Long-term Temporal Structure
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#visualizing-video-models">
<span class="md-ellipsis">
      Visualizing Video Models
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#multimodal-video-understanding">
<span class="md-ellipsis">
      Multimodal Video Understanding
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#13-generative-models">
<span class="md-ellipsis">
      13 - Generative Models
    </span>
</a>
<nav aria-label="13 - Generative Models" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#pixelrnn-and-pixelcnn">
<span class="md-ellipsis">
      PixelRNN and PixelCNN
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#variational-autoencoder">
<span class="md-ellipsis">
      Variational Autoencoder
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#generative-adversarial-networks-gans">
<span class="md-ellipsis">
      Generative Adversarial Networks (GANs)
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#14-self-supervised-learning">
<span class="md-ellipsis">
      14 - Self-supervised Learning
    </span>
</a>
<nav aria-label="14 - Self-supervised Learning" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#pretext-tasks">
<span class="md-ellipsis">
      Pretext Tasks
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#contrastive-representation-learning">
<span class="md-ellipsis">
      Contrastive Representation Learning
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#15-low-level-vision">
<span class="md-ellipsis">
      15 - Low-Level Vision
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#16-3d-vision">
<span class="md-ellipsis">
      16 - 3D Vision
    </span>
</a>
<nav aria-label="16 - 3D Vision" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#representation">
<span class="md-ellipsis">
      Representation
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#ai-3d">
<span class="md-ellipsis">
      AI + 3D
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/WncFht/test/edit/main/docs/AI/CS231n/CS231n_notes.md" title="edit.link.title">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"></path></svg>
</a>
<h1 id="computer-vision">Computer Vision<a class="headerlink" href="#computer-vision" title="Permanent link">¶</a></h1>
<div style="margin-top: -30px; font-size: 0.75em; opacity: 0.7;">
<p><span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> 约<span class="heti-skip"><span class="heti-spacing"> </span>8852<span class="heti-spacing"> </span></span>个字 <span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 17H7V3h14m0-2H7a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V3a2 2 0 0 0-2-2M3 5H1v16a2 2 0 0 0 2 2h16v-2H3m12.96-10.71-2.75 3.54-1.96-2.36L8.5 15h11z"></path></svg></span> <span>167<span class="heti-spacing"> </span></span>张图片 <span class="twemoji"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> 预计阅读时间<span class="heti-skip"><span class="heti-spacing"> </span>30<span class="heti-spacing"> </span></span>分钟</p>
</div>
<p><span>This note is based on<span class="heti-spacing"> </span></span><a href="https://github.com/DaizeDong/Stanford-CS231n-2021-and-2022/"><span>GitHub - DaizeDong/Stanford-CS231n-2021-and-2022: Notes and slides for Stanford CS231n 2021 &amp; 2022 in English. I merged the contents together to get a better version. Assignments are not included.<span class="heti-spacing"> </span></span>斯坦福<span class="heti-skip"><span class="heti-spacing"> </span>cs231n<span class="heti-spacing"> </span></span>的课程笔记<span class="heti-skip"><span class="heti-spacing"> </span>(<span class="heti-spacing"> </span></span>英文版本，不含实验代码<span><span class="heti-spacing"> </span>)</span>，将<span class="heti-skip"><span class="heti-spacing"> </span>2021<span class="heti-spacing"> </span></span>与<span class="heti-skip"><span class="heti-spacing"> </span>2022<span class="heti-spacing"> </span></span>两年的课程进行了合并，分享以供交流。</a><br/>
And I will add some blogs, articles and other understanding.</p>
<table>
<thead>
<tr>
<th>Topic</th>
<th>Chapter</th>
</tr>
</thead>
<tbody>
<tr>
<td>Deep Learning Basics</td>
<td>2 - 4</td>
</tr>
<tr>
<td>Perceiving and Understanding the Visual World</td>
<td>5 - 12</td>
</tr>
<tr>
<td>Reconstructing and Interacting with the Visual World</td>
<td>13 - 16</td>
</tr>
<tr>
<td>Human-Centered Applications and Implications</td>
<td>17 - 18</td>
</tr>
</tbody>
</table>
<h2 id="1-introduction">1 - Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">¶</a></h2>
<p>A brief history of computer vision &amp; deep learning...</p>
<h2 id="2-image-classification">2 - Image Classification<a class="headerlink" href="#2-image-classification" title="Permanent link">¶</a></h2>
<p><strong>Image Classification:</strong> A core task in Computer Vision. The main drive to the progress of CV.</p>
<p><strong>Challenges:</strong> Viewpoint variation, background clutter, illumination, occlusion, deformation, intra-class variation...</p>
<h3 id="k-nearest-neighbor">K Nearest Neighbor<a class="headerlink" href="#k-nearest-neighbor" title="Permanent link">¶</a></h3>
<p><strong>Hyperparameters:</strong> Distance metric (<span class="arithmatex">\(p\)</span> norm), <span class="arithmatex">\(k\)</span> number.</p>
<p>Choose hyperparameters using validation set.</p>
<p>Never use k-Nearest Neighbor with pixel distance.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/2-cross_validation.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/2-cross_validation.png"/></a></p>
<h3 id="linear-classifier">Linear Classifier<a class="headerlink" href="#linear-classifier" title="Permanent link">¶</a></h3>
<p>Pass...</p>
<h2 id="3-loss-functions-and-optimization">3 - Loss Functions and Optimization<a class="headerlink" href="#3-loss-functions-and-optimization" title="Permanent link">¶</a></h2>
<h3 id="loss-functions">Loss Functions<a class="headerlink" href="#loss-functions" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Dataset</th>
<th><span class="arithmatex">\(\big\{(x_i,y_i)\big\}_{i=1}^N\\\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>Loss Function</td>
<td><span class="arithmatex">\(L=\frac{1}{N}\sum_{i=1}^NL_i\big(f(x_i,W),y_i\big)\\\)</span></td>
</tr>
<tr>
<td>Loss Function with Regularization</td>
<td><span class="arithmatex">\(L=\frac{1}{N}\sum_{i=1}^NL_i\big(f(x_i,W),y_i\big)+\lambda R(W)\\\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Motivation:</strong> Want to interpret raw classifier scores as probabilities.</p>
<table>
<thead>
<tr>
<th>Softmax Classifier</th>
<th><span class="arithmatex">\(p_i=Softmax(y_i)=\frac{\exp(y_i)}{\sum_{j=1}^N\exp(y_j)}\\\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>Cross Entropy Loss</td>
<td><span class="arithmatex">\(L_i=-y_i\log p_i\\\)</span></td>
</tr>
<tr>
<td>Cross Entropy Loss with Regularization</td>
<td><span class="arithmatex">\(L=-\frac{1}{N}\sum_{i=1}^Ny_i\log p_i+\lambda R(W)\\\)</span></td>
</tr>
</tbody>
</table>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-loss.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-loss.png"/></a></p>
<h3 id="optimization">Optimization<a class="headerlink" href="#optimization" title="Permanent link">¶</a></h3>
<h4 id="sgd-with-momentum">SGD with Momentum<a class="headerlink" href="#sgd-with-momentum" title="Permanent link">¶</a></h4>
<p><strong>Problems that SGD can't handle:</strong></p>
<ol>
<li>Inequality of gradient in different directions.</li>
<li>Local minima and saddle point (much more common in high dimension).</li>
<li>Noise of gradient from mini-batch.</li>
</ol>
<p><strong>Momentum:</strong> Build up “velocity” <span class="arithmatex">\(v_t\)</span> as a running mean of gradients.</p>
<table>
<thead>
<tr>
<th>SGD</th>
<th>SGD + Momentum</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(x_{t+1}=x_t-\alpha\nabla f(x_t)\)</span></td>
<td><span class="arithmatex">\(\begin{align}&amp;v_{t+1}=\rho v_t+\nabla f(x_t)\\&amp;x_{t+1}=x_t-\alpha v_{t+1}\end{align}\)</span></td>
</tr>
<tr>
<td>Naive gradient descent.</td>
<td><span class="arithmatex">\(\rho\)</span> gives "friction", typically <span class="arithmatex">\(\rho=0.9,0.99,0.999,...\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Nesterov Momentum:</strong> Use the derivative on point <span class="arithmatex">\(x_t+\rho v_t\)</span> as gradient instead point <span class="arithmatex">\(x_t\)</span>.</p>
<table>
<thead>
<tr>
<th>Momentum</th>
<th>Nesterov Momentum</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\begin{align}&amp;v_{t+1}=\rho v_t+\nabla f(x_t)\\&amp;x_{t+1}=x_t-\alpha v_{t+1}\end{align}\)</span></td>
<td><span class="arithmatex">\(\begin{align}&amp;v_{t+1}=\rho v_t+\nabla f(x_t+\rho v_t)\\&amp;x_{t+1}=x_t-\alpha v_{t+1}\end{align}\)</span></td>
</tr>
<tr>
<td>Use gradient at current point.</td>
<td>Look ahead for the gradient in velocity direction.</td>
</tr>
</tbody>
</table>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-momentum.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-momentum.png"/></a></p>
<h4 id="adagrad-and-rmsprop">AdaGrad and RMSProp<a class="headerlink" href="#adagrad-and-rmsprop" title="Permanent link">¶</a></h4>
<p><strong>AdaGrad:</strong> Accumulate squared gradient, and gradually decrease the step size.</p>
<p><strong>RMSProp:</strong> Accumulate squared gradient while decaying former ones, and gradually decrease the step size. ("Leaky AdaGrad")</p>
<table>
<thead>
<tr>
<th>AdaGrad</th>
<th>RMSProp</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\begin{align}\text{Initialize:}&amp;\\&amp;r:=0\\\text{Update:}&amp;\\&amp;r:=r+\Big[\nabla f(x_t)\Big]^2\\&amp;x_{t+1}=x_t-\alpha\frac{\nabla f(x_t)}{\sqrt{r}}\end{align}\)</span></td>
<td><span class="arithmatex">\(\begin{align}\text{Initialize:}&amp;\\&amp;r:=0\\\text{Update:}&amp;\\&amp;r:=\rho r+(1-\rho)\Big[\nabla f(x_t)\Big]^2\\&amp;x_{t+1}=x_t-\alpha\frac{\nabla f(x_t)}{\sqrt{r}}\end{align}\)</span></td>
</tr>
<tr>
<td>Continually accumulate squared gradients.</td>
<td><span class="arithmatex">\(\rho\)</span> gives "decay rate", typically <span class="arithmatex">\(\rho=0.9,0.99,0.999,...\)</span></td>
</tr>
</tbody>
</table>
<h4 id="adam">Adam<a class="headerlink" href="#adam" title="Permanent link">¶</a></h4>
<p>Sort of like "RMSProp + Momentum".</p>
<table>
<thead>
<tr>
<th>Adam (simple version)</th>
<th>Adam (full version)</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\begin{align}\text{Initialize:}&amp;\\&amp;r_1:=0\\&amp;r_2:=0\\\text{Update:}&amp;\\&amp;r_1:=\beta_1r_1+(1-\beta_1)\nabla f(x_t)\\&amp;r_2:=\beta_2r_2+(1-\beta_2)\Big[\nabla f(x_t)\Big]^2\\&amp;x_{t+1}=x_t-\alpha\frac{r_1}{\sqrt{r_2}}\end{align}\)</span></td>
<td><span class="arithmatex">\(\begin{align}\text{Initialize:}\\&amp;r_1:=0\\&amp;r_2:=0\\\text{For }i\text{:}\\&amp;r_1:=\beta_1r_1+(1-\beta_1)\nabla f(x_t)\\&amp;r_2:=\beta_2r_2+(1-\beta_2)\Big[\nabla f(x_t)\Big]^2\\&amp;r_1'=\frac{r_1}{1-\beta_1^i}\\&amp;r_2'=\frac{r_2}{1-\beta_2^i}\\&amp;x_{t+1}=x_t-\alpha\frac{r_1'}{\sqrt{r_2'}}\end{align}\)</span></td>
</tr>
<tr>
<td>Build up “velocity” for both gradient and squared gradient.</td>
<td>Correct the "bias" that <span class="arithmatex">\(r_1=r_2=0\)</span> for the first few iterations.</td>
</tr>
</tbody>
</table>
<h4 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">¶</a></h4>
<table>
<thead>
<tr>
<th style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-optimization_overview.gif"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-optimization_overview.gif"/></a></th>
<th style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-optimization_overview2.gif"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-optimization_overview2.gif"/></a></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="learning-rate-decay">Learning Rate Decay<a class="headerlink" href="#learning-rate-decay" title="Permanent link">¶</a></h4>
<p>Reduce learning rate at a few fixed points to get a better convergence over time.</p>
<p><span class="arithmatex">\(\alpha_0\)</span> : Initial learning rate.</p>
<p><span class="arithmatex">\(\alpha_t\)</span> : Learning rate in epoch <span class="arithmatex">\(t\)</span>.</p>
<p><span class="arithmatex">\(T\)</span> : Total number of epochs.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Equation</th>
<th>Picture</th>
</tr>
</thead>
<tbody>
<tr>
<td>Step</td>
<td>Reduce <span class="arithmatex">\(\alpha_t\)</span> constantly  in a fixed step.</td>
<td><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_step.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_step.png"/></a></td>
</tr>
<tr>
<td>Cosine</td>
<td><span class="arithmatex">\(\begin{align}\alpha_t=\frac{1}{2}\alpha_0\Bigg[1+\cos(\frac{t\pi}{T})\Bigg]\end{align}\)</span></td>
<td><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_cosine.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_cosine.png"/></a></td>
</tr>
<tr>
<td>Linear</td>
<td><span class="arithmatex">\(\begin{align}\alpha_t=\alpha_0\Big(1-\frac{t}{T}\Big)\end{align}\)</span></td>
<td><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_linear.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_linear.png"/></a></td>
</tr>
<tr>
<td>Inverse Sqrt</td>
<td><span class="arithmatex">\(\begin{align}\alpha_t=\frac{\alpha_0}{\sqrt{t}}\end{align}\)</span></td>
<td><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_sqrt.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_sqrt.png"/></a></td>
</tr>
</tbody>
</table>
<p>High initial learning rates can make loss explode, linearly increasing learning rate in the first few iterations can prevent this.</p>
<p><strong>Learning rate warm up:</strong></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_increase.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-learning_rate_increase.png"/></a></p>
<p><strong>Empirical rule of thumb:</strong> If you increase the batch size by <span class="arithmatex">\(N\)</span>, also scale the initial learning rate by <span class="arithmatex">\(N\)</span> .</p>
<h4 id="second-order-optimization">Second-Order Optimization<a class="headerlink" href="#second-order-optimization" title="Permanent link">¶</a></h4>
<table>
<thead>
<tr>
<th></th>
<th>Picture</th>
<th>Time Complexity</th>
<th>Space Complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>First Order</td>
<td><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-first_order.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-first_order.png"/></a></td>
<td><span class="arithmatex">\(O(n)\)</span></td>
<td><span class="arithmatex">\(O(n)\)</span></td>
</tr>
<tr>
<td>Second Order</td>
<td><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-second_order.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/3-second_order.png"/></a></td>
<td><span class="arithmatex">\(O(n^2)\)</span> with <strong>BGFS</strong> optimization</td>
<td><span class="arithmatex">\(O(n)\)</span> with <strong>L-BGFS</strong> optimization</td>
</tr>
</tbody>
</table>
<p><strong>L-BGFS :</strong> Limited memory BGFS.</p>
<ol>
<li>Works very well in full batch, deterministic <span class="arithmatex">\(f(x)\)</span>.</li>
<li>Does not transfer very well to mini-batch setting.</li>
</ol>
<h4 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">¶</a></h4>
<table>
<thead>
<tr>
<th>Method</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Adam</td>
<td>Often chosen as default method.<br/>Work ok even with constant learning rate.</td>
</tr>
<tr>
<td>SGD + Momentum</td>
<td>Can outperform Adam.<br/>Require more tuning of learning rate and schedule.</td>
</tr>
<tr>
<td>L-BGFS</td>
<td>If can afford to do full batch updates then try out.</td>
</tr>
</tbody>
</table>
<ul>
<li>An article about gradient descent: <a href="https://arxiv.org/pdf/1609.04747">Anoverview of gradient descent optimization algorithms</a></li>
<li>A blog: <a href="https://johnchenresearch.github.io/demon/">An updated overview of recent gradient descent algorithms – John Chen – ML at Rice University</a></li>
</ul>
<h2 id="4-neural-networks-and-backpropagation">4 - Neural Networks and Backpropagation<a class="headerlink" href="#4-neural-networks-and-backpropagation" title="Permanent link">¶</a></h2>
<h3 id="neural-networks">Neural Networks<a class="headerlink" href="#neural-networks" title="Permanent link">¶</a></h3>
<p><strong>Motivation:</strong> Inducted bias can appear to be high when using human-designed features.</p>
<p><strong>Activation:</strong> Sigmoid, tanh, ReLU, LeakyReLU...</p>
<p><strong>Architecture:</strong> Input layer, hidden layer, output layer.</p>
<p><strong>Do not use the size of a neural network as the regularizer. Use regularization instead!</strong></p>
<p><strong>Gradient Calculation:</strong> Computational Graph + Backpropagation.</p>
<h3 id="backpropagation">Backpropagation<a class="headerlink" href="#backpropagation" title="Permanent link">¶</a></h3>
<p>Using Jacobian matrix to calculate the gradient of each node in a computation graph.</p>
<p>Suppose that we have a computation flow like this:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/4-graph.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/4-graph.png"/></a></p>
<table>
<thead>
<tr>
<th>Input X</th>
<th>Input W</th>
<th>Output Y</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(X=\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}\)</span></td>
<td><span class="arithmatex">\(W=\begin{bmatrix}w_{11}&amp;w_{12}&amp;\cdots&amp;w_{1n}\\w_{21}&amp;w_{22}&amp;\cdots&amp;w_{2n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\w_{m1}&amp;w_{m2}&amp;\cdots&amp;w_{mn}\end{bmatrix}\)</span></td>
<td><span class="arithmatex">\(Y=\begin{bmatrix}y_1\\y_2\\\vdots\\y_m\end{bmatrix}\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(n\times 1\)</span></td>
<td><span class="arithmatex">\(m\times n\)</span></td>
<td><span class="arithmatex">\(m\times 1\)</span></td>
</tr>
</tbody>
</table>
<p>After applying feed forward, we can calculate gradients like this:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/4-graph2.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/4-graph2.png"/></a></p>
<table>
<thead>
<tr>
<th>Derivative Matrix of X</th>
<th>Jacobian Matrix of X</th>
<th>Derivative Matrix of Y</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(D_X=\begin{bmatrix}\frac{\partial L}{\partial x_1}\\\frac{\partial L}{\partial x_2}\\\vdots\\\frac{\partial L}{\partial x_n}\end{bmatrix}\)</span></td>
<td><span class="arithmatex">\(J_X=\begin{bmatrix}\frac{\partial y_1}{\partial x_1}&amp;\frac{\partial y_1}{\partial x_2}&amp;\cdots&amp;\frac{\partial y_1}{\partial x_n}\\\frac{\partial y_2}{\partial x_1}&amp;\frac{\partial y_2}{\partial x_2}&amp;\cdots&amp;\frac{\partial y_2}{\partial x_n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial y_m}{\partial x_1}&amp;\frac{\partial y_m}{\partial x_2}&amp;\cdots&amp;\frac{\partial y_m}{\partial x_n}\end{bmatrix}\)</span></td>
<td><span class="arithmatex">\(D_Y=\begin{bmatrix}\frac{\partial L}{\partial y_1}\\\frac{\partial L}{\partial y_2}\\\vdots\\\frac{\partial L}{\partial y_m}\end{bmatrix}\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(n\times 1\)</span></td>
<td><span class="arithmatex">\(m\times n\)</span></td>
<td><span class="arithmatex">\(m\times 1\)</span></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Derivative Matrix of W</th>
<th>Jacobian Matrix of W</th>
<th>Derivative Matrix of Y</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(W=\begin{bmatrix}\frac{\partial L}{\partial w_{11}}&amp;\frac{\partial L}{\partial w_{12}}&amp;\cdots&amp;\frac{\partial L}{\partial w_{1n}}\\\frac{\partial L}{\partial w_{21}}&amp;\frac{\partial L}{\partial w_{22}}&amp;\cdots&amp;\frac{\partial L}{\partial w_{2n}}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial L}{\partial w_{m1}}&amp;\frac{\partial L}{\partial w_{m2}}&amp;\cdots&amp;\frac{\partial L}{\partial w_{mn}}\end{bmatrix}\)</span></td>
<td><span class="arithmatex">\(J_W^{(k)}=\begin{bmatrix}\frac{\partial y_k}{\partial w_{11}}&amp;\frac{\partial y_k}{\partial w_{12}}&amp;\cdots&amp;\frac{\partial y_k}{\partial w_{1n}}\\\frac{\partial y_k}{\partial w_{21}}&amp;\frac{\partial y_k}{\partial w_{22}}&amp;\cdots&amp;\frac{\partial y_k}{\partial w_{2n}}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial y_k}{\partial w_{m1}}&amp;\frac{\partial y_k}{\partial w_{m2}}&amp;\cdots&amp;\frac{\partial y_k}{\partial w_{mn}}\end{bmatrix}\)</span><br/><span class="arithmatex">\(J_W=\begin{bmatrix}J_W^{(1)}&amp;J_W^{(2)}&amp;\cdots&amp;J_W^{(m)}\end{bmatrix}\)</span></td>
<td><span class="arithmatex">\(D_Y=\begin{bmatrix}\frac{\partial L}{\partial y_1}\\\frac{\partial L}{\partial y_2}\\\vdots\\\frac{\partial L}{\partial y_m}\end{bmatrix}\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(m\times n\)</span></td>
<td><span class="arithmatex">\(m\times m\times n\)</span></td>
<td>$ m\times 1$</td>
</tr>
</tbody>
</table>
<p>For each element in <span class="arithmatex">\(D_X\)</span> , we have:</p>
<p><span class="arithmatex">\(D_{Xi}=\frac{\partial L}{\partial x_i}=\sum_{j=1}^m\frac{\partial L}{\partial y_j}\frac{\partial y_j}{\partial x_i}\\\)</span></p>
<h2 id="5-convolutional-neural-networks">5 - Convolutional Neural Networks<a class="headerlink" href="#5-convolutional-neural-networks" title="Permanent link">¶</a></h2>
<h3 id="convolution-layer">Convolution Layer<a class="headerlink" href="#convolution-layer" title="Permanent link">¶</a></h3>
<h4 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">¶</a></h4>
<p><strong>Convolve a filter with an image:</strong> Slide the filter spatially within the image, computing dot products in each region.</p>
<p>Giving a <span class="arithmatex">\(32\times32\times3\)</span> image and a <span class="arithmatex">\(5\times5\times3\)</span> filter, a convolution looks like:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution.png"/></a></p>
<p>Convolve six <span class="arithmatex">\(5\times5\times3\)</span> filters to a <span class="arithmatex">\(32\times32\times3\)</span> image with step size <span class="arithmatex">\(1\)</span>, we can get a <span class="arithmatex">\(28\times28\times6\)</span> feature:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_six_filters.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_six_filters.png"/></a></p>
<p>With an activation function after each convolution layer, we can build the ConvNet with a sequence of convolution layers:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_net.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_net.png"/></a></p>
<p>By <strong>changing the step size</strong> between each move for filters, or <strong>adding zero-padding</strong> around the image, we can modify the size of the output:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_padding.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_padding.png"/></a></p>
<h4 id="1times1-convolution-layer"><span class="arithmatex">\(1\times1\)</span> Convolution Layer<a class="headerlink" href="#1times1-convolution-layer" title="Permanent link">¶</a></h4>
<p>This kind of layer makes perfect sense. It is usually used to change the dimension (channel) of features.</p>
<p>A <span class="arithmatex">\(1\times1\)</span> convolution layer can also be treated as a full-connected linear layer.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_1times1.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-convolution_1times1.png"/></a></p>
<h4 id="summary_1">Summary<a class="headerlink" href="#summary_1" title="Permanent link">¶</a></h4>
<table>
<thead>
<tr>
<th><strong>Input</strong></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>image size</td>
<td><span class="arithmatex">\(W_1\times H_1\times C\)</span></td>
</tr>
<tr>
<td>filter size</td>
<td><span class="arithmatex">\(F\times F\times C\)</span></td>
</tr>
<tr>
<td>filter number</td>
<td><span class="arithmatex">\(K\)</span></td>
</tr>
<tr>
<td>stride</td>
<td><span class="arithmatex">\(S\)</span></td>
</tr>
<tr>
<td>zero padding</td>
<td><span class="arithmatex">\(P\)</span></td>
</tr>
<tr>
<td><strong>Output</strong></td>
<td></td>
</tr>
<tr>
<td>output size</td>
<td><span class="arithmatex">\(W_2\times H_2\times K\)</span></td>
</tr>
<tr>
<td>output width</td>
<td><span class="arithmatex">\(W_2=\frac{W_1-F+2P}{S}+1\\\)</span></td>
</tr>
<tr>
<td>output height</td>
<td><span class="arithmatex">\(H_2=\frac{H_1-F+2P}{S}+1\\\)</span></td>
</tr>
<tr>
<td><strong>Parameters</strong></td>
<td></td>
</tr>
<tr>
<td>parameter number (weight)</td>
<td><span class="arithmatex">\(F^2CK\)</span></td>
</tr>
<tr>
<td>parameter number (bias)</td>
<td><span class="arithmatex">\(K\)</span></td>
</tr>
</tbody>
</table>
<h3 id="pooling-layer">Pooling layer<a class="headerlink" href="#pooling-layer" title="Permanent link">¶</a></h3>
<p>Make the representations smaller and more manageable.</p>
<p><strong>An example of max pooling:</strong></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-pooling.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-pooling.png"/></a></p>
<table>
<thead>
<tr>
<th><strong>Input</strong></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>image size</td>
<td><span class="arithmatex">\(W_1\times H_1\times C\)</span></td>
</tr>
<tr>
<td>spatial extent</td>
<td><span class="arithmatex">\(F\times F\)</span></td>
</tr>
<tr>
<td>stride</td>
<td><span class="arithmatex">\(S\)</span></td>
</tr>
<tr>
<td><strong>Output</strong></td>
<td></td>
</tr>
<tr>
<td>output size</td>
<td><span class="arithmatex">\(W_2\times H_2\times C\)</span></td>
</tr>
<tr>
<td>output width</td>
<td><span class="arithmatex">\(W_2=\frac{W_1-F}{S}+1\\\)</span></td>
</tr>
<tr>
<td>output height</td>
<td><span class="arithmatex">\(H_2=\frac{H_1-F}{S}+1\\\)</span></td>
</tr>
</tbody>
</table>
<h3 id="convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)<a class="headerlink" href="#convolutional-neural-networks-cnn" title="Permanent link">¶</a></h3>
<p>CNN stack CONV, POOL, FC layers.</p>
<p><strong>CNN Trends:</strong></p>
<ol>
<li>Smaller filters and deeper architectures.</li>
<li>Getting rid of POOL/FC layers (just CONV).</li>
</ol>
<p><strong>Historically architectures of CNN looked like:</strong></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-model_history.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/5-model_history.png"/></a></p>
<p>where usually <span class="arithmatex">\(m\)</span> is large, <span class="arithmatex">\(0\le n\le5\)</span>, <span class="arithmatex">\(0\le k\le2\)</span>.</p>
<p>Recent advances such as <strong>ResNet</strong> / <strong>GoogLeNet</strong> have challenged this paradigm.</p>
<h2 id="6-cnn-architectures">6 - CNN Architectures<a class="headerlink" href="#6-cnn-architectures" title="Permanent link">¶</a></h2>
<p>Best model in ImageNet competition:</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-image_net.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-image_net.png"/></a></p>
<h3 id="alexnet">AlexNet<a class="headerlink" href="#alexnet" title="Permanent link">¶</a></h3>
<p>8 layers.</p>
<p>First use of ConvNet in image classification problem.</p>
<p>Filter size decreases in deeper layer.</p>
<p>Channel number increases in deeper layer.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-alexnet.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-alexnet.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-alexnet_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-alexnet_p.png"/></a></p>
<h3 id="vgg">VGG<a class="headerlink" href="#vgg" title="Permanent link">¶</a></h3>
<p>19 layers. (also provide 16 layers edition)</p>
<p>Static filter size (<span class="arithmatex">\(3\times3\)</span>) in all layers:</p>
<ol>
<li>The effective receptive field expands with the layer gets deeper.</li>
<li>Deeper architecture gets more non-linearities and few parameters.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg_field.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg_field.png"/></a></p>
<p>Most memory is in early convolution layers.</p>
<p>Most parameter is in late FC layers.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-vgg_p.png"/></a></p>
<h3 id="googlenet">GoogLeNet<a class="headerlink" href="#googlenet" title="Permanent link">¶</a></h3>
<p>22 layers.</p>
<p>No FC layers, only 5M parameters. ( <span class="arithmatex">\(8.3\%\)</span> of AlexNet, <span class="arithmatex">\(3.7\%\)</span> of VGG )</p>
<p>Devise efficient "inception module".</p>
<h4 id="inception-module">Inception Module<a class="headerlink" href="#inception-module" title="Permanent link">¶</a></h4>
<p>Design a good local network topology (network within a network) and then stack these modules on top of each other.</p>
<p><strong>Naive Inception Module:</strong></p>
<ol>
<li>Apply parallel filter operations on the input from previous layer.</li>
<li>Concatenate all filter outputs together channel-wise.</li>
<li><strong>Problem:</strong> The depth (channel number) increases too fast, costing expensive computation.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_inception.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_inception.png"/></a></p>
<p><strong>Inception Module with Dimension Reduction:</strong></p>
<ol>
<li>Add "bottle neck" layers to reduce the dimension.</li>
<li>Also get fewer computation cost.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_inception_revised.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_inception_revised.png"/></a></p>
<h4 id="architecture">Architecture<a class="headerlink" href="#architecture" title="Permanent link">¶</a></h4>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_p.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_p2.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-googlenet_p2.png"/></a></p>
<h3 id="resnet">ResNet<a class="headerlink" href="#resnet" title="Permanent link">¶</a></h3>
<p>152 layers for ImageNet.</p>
<p>Devise "residual connections".</p>
<p>Use BN in place of dropout.</p>
<h4 id="residual-connections">Residual Connections<a class="headerlink" href="#residual-connections" title="Permanent link">¶</a></h4>
<p><strong>Hypothesis:</strong> Deeper models have more representation power than shallow ones. But they are harder to optimize.</p>
<p><strong>Solution:</strong> Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_residual.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_residual.png"/></a></p>
<p>It is necessary to use ReLU as activation function, in order to apply identity mapping when <span class="arithmatex">\(F(x)=0\)</span> .</p>
<h4 id="architecture_1">Architecture<a class="headerlink" href="#architecture_1" title="Permanent link">¶</a></h4>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_train.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_train.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-resnet_p.png"/></a></p>
<h3 id="senet">SENet<a class="headerlink" href="#senet" title="Permanent link">¶</a></h3>
<p>Using ResNeXt-152 as a base architecture.</p>
<p>Add a “feature recalibration” module. <strong>(adjust weights of each channel)</strong></p>
<p>Using the <strong>global avg-pooling layer</strong> + <strong>FC layers</strong> to determine feature map weights.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-senet_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-senet_p.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-senet_p2.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/6-senet_p2.png"/></a></p>
<h3 id="improvements-of-resnet">Improvements of ResNet<a class="headerlink" href="#improvements-of-resnet" title="Permanent link">¶</a></h3>
<p>Wide Residual Networks, ResNeXt, DenseNet, MobileNets...</p>
<h3 id="other-interesting-networks">Other Interesting Networks<a class="headerlink" href="#other-interesting-networks" title="Permanent link">¶</a></h3>
<p><strong>NASNet:</strong> Neural Architecture Search with Reinforcement Learning.</p>
<p><strong>EfficientNet:</strong> Smart Compound Scaling.</p>
<h2 id="7-training-neural-networks">7 - Training Neural Networks<a class="headerlink" href="#7-training-neural-networks" title="Permanent link">¶</a></h2>
<h3 id="activation-functions">Activation Functions<a class="headerlink" href="#activation-functions" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Activation</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid, tanh</td>
<td>Do not use.</td>
</tr>
<tr>
<td>ReLU</td>
<td>Use as default.</td>
</tr>
<tr>
<td>Leaky ReLU, Maxout, ELU, SELU</td>
<td>Replace ReLU to squeeze out some marginal gains.</td>
</tr>
<tr>
<td>Swish</td>
<td>No clear usage.</td>
</tr>
</tbody>
</table>
<h3 id="data-processing">Data Processing<a class="headerlink" href="#data-processing" title="Permanent link">¶</a></h3>
<p>Apply centralization and normalization before training.</p>
<p>In practice for pictures, usually we apply channel-wise centralization only.</p>
<h3 id="weight-initialization">Weight Initialization<a class="headerlink" href="#weight-initialization" title="Permanent link">¶</a></h3>
<p>Assume that we have 6 layers in a network.</p>
<p><span class="arithmatex">\(D_i\)</span> : input size of layer <span class="arithmatex">\(i\)</span></p>
<p><span class="arithmatex">\(W_i\)</span> : weights in layer <span class="arithmatex">\(i\)</span></p>
<p><span class="arithmatex">\(X_i\)</span> : output after activation of layer <span class="arithmatex">\(i\)</span>, we have <span class="arithmatex">\(X_i=g(Z_i)=g(W_iX_{i-1}+B_i)\)</span></p>
<p><strong>We initialize each parameter in <span class="arithmatex">\(W_i\)</span> randomly in <span class="arithmatex">\([-k_i,k_i]\)</span> .</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tanh Activation</th>
<th style="text-align: center;">Output Distribution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><span class="arithmatex">\(k_i=0.01\)</span></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_0.01.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_0.01.png"/></a></td>
</tr>
<tr>
<td style="text-align: center;"><span class="arithmatex">\(k_i=0.05\)</span></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_0.05.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_0.05.png"/></a></td>
</tr>
<tr>
<td style="text-align: center;"><strong>Xavier Initialization</strong> <span class="arithmatex">\(k_i=\frac{1}{\sqrt{D_i}\\}\)</span></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_xavier.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-sigmoid_xavier.png"/></a></td>
</tr>
</tbody>
</table>
<p>When <span class="arithmatex">\(k_i=0.01\)</span>, the variance keeps decreasing as the layer gets deeper. As a result, the output of each neuron in deep layer will all be 0. The partial derivative <span class="arithmatex">\(\frac{\partial Z_i}{\partial W_i}=X_{i-1}=0\\\)</span>. (no gradient)</p>
<p>When <span class="arithmatex">\(k_i=0.05\)</span>, most neurons is saturated. The partial derivative <span class="arithmatex">\(\frac{\partial X_i}{\partial Z_i}=g'(Z_i)=0\\\)</span>. (no gradient)</p>
<p><strong>To solve this problem, We need to keep the variance same in each layer.</strong></p>
<p>Assuming that <span class="arithmatex">\(Var\big(X_{i-1}^{(1)}\big)=Var\big(X_{i-1}^{(2)}\big)=\dots=Var\big(X_{i-1}^{(D_i)}\big)\)</span></p>
<p>We have <span class="arithmatex">\(Z_i=X_{i-1}^{(1)}W_i^{(:,1)}+X_{i-1}^{(2)}W_i^{(:,2)}+\dots+X_{i-1}^{(D_i)}W_i^{(:,D_i)}=\sum_{n=1}^{D_i}X_{i-1}^{(n)}W_i^{(:,n)}\\\)</span></p>
<p>We want <span class="arithmatex">\(Var\big(Z_i\big)=Var\big(X_{i-1}^{(n)}\big)\)</span></p>
<p><strong>Let's do some conduction:</strong></p>
<p><span class="arithmatex">\(\begin{aligned}Var\big(Z_i\big)&amp;=Var\Bigg(\sum_{n=1}^{D_i}X_{i-1}^{(n)}W_i^{(:,n)}\Bigg)\\&amp;=D_i\ Var\Big(X_{i-1}^{(n)}W_i^{(:,n)}\Big)\\&amp;=D_i\ Var\Big(X_{i-1}^{(n)}\Big)\ Var\Big(W_i^{(:,n)}\Big)\end{aligned}\)</span></p>
<p>So <span class="arithmatex">\(Var\big(Z_i\big)=Var\big(X_{i-1}^{(n)}\big)\)</span> only when <span class="arithmatex">\(Var\Big(W_i^{(:,n)}\Big)=\frac{1}{D_i}\\\)</span>, that is to say <span class="arithmatex">\(k_i=\frac{1}{\sqrt{D_i}}\\\)</span></p>
<table>
<thead>
<tr>
<th style="text-align: center;">ReLU Activation</th>
<th style="text-align: center;">Output Distribution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><strong>Xavier Initialization</strong> <span class="arithmatex">\(k_i=\frac{1}{\sqrt{D_i}\\}\)</span></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-relu_xavier.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-relu_xavier.png"/></a></td>
</tr>
<tr>
<td style="text-align: center;"><strong>Kaiming Initialization</strong> <span class="arithmatex">\(k_i=\sqrt{2D_i}\)</span></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-relu_kaiming.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-relu_kaiming.png"/></a></td>
</tr>
</tbody>
</table>
<p>For ReLU activation, when using xavier initialization, there still exist "variance decreasing" problem.</p>
<p>We can use kaiming initialization instead to fix this.</p>
<h3 id="batch-normalization">Batch Normalization<a class="headerlink" href="#batch-normalization" title="Permanent link">¶</a></h3>
<p>Force the inputs to be "nicely scaled" at each layer.</p>
<p><span class="arithmatex">\(N\)</span> : batch size</p>
<p><span class="arithmatex">\(D\)</span> : feature size</p>
<p><span class="arithmatex">\(x\)</span> : input with shape <span class="arithmatex">\(N\times D\)</span> </p>
<p><span class="arithmatex">\(\gamma\)</span> : learnable scale and shift parameter with shape <span class="arithmatex">\(D\)</span></p>
<p><span class="arithmatex">\(\beta\)</span> : learnable scale and shift parameter with shape <span class="arithmatex">\(D\)</span></p>
<p><strong>The procedure of batch normalization:</strong></p>
<ol>
<li>Calculate channel-wise mean <span class="arithmatex">\(\mu_j=\frac{1}{N}\sum_{i=1}^Nx_{i,j}\\\)</span> . The result <span class="arithmatex">\(\mu\)</span> with shape <span class="arithmatex">\(D\)</span> .</li>
<li>Calculate channel-wise variance <span class="arithmatex">\(\sigma_j^2=\frac{1}{N}\sum_{i=1}^N(x_{i,j}-\mu_j)^2\\\)</span> . The result <span class="arithmatex">\(\sigma^2\)</span> with shape <span class="arithmatex">\(D\)</span> .</li>
<li>Calculate normalized <span class="arithmatex">\(\hat{x}_{i,j}=\frac{x_{i,j}-\mu_j}{\sqrt{\sigma_j^2+\epsilon}}\\\)</span> . The result <span class="arithmatex">\(\hat{x}\)</span> with shape <span class="arithmatex">\(N\times D\)</span> .</li>
<li>Scale normalized input to get output <span class="arithmatex">\(y_{i,j}=\gamma_j\hat{x}_{i,j}+\beta_j\)</span> . The result <span class="arithmatex">\(y\)</span> with shape <span class="arithmatex">\(N\times D\)</span> .</li>
</ol>
<p><strong>Why scale:</strong> The constraint "zero-mean, unit variance" may be too hard.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-batch_norm.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-batch_norm.png"/></a></p>
<p><strong>Pros:</strong></p>
<ol>
<li>Makes deep networks much easier to train!</li>
<li>Improves gradient flow.</li>
<li>Allows higher learning rates, faster convergence.</li>
<li>Networks become more robust to initialization.</li>
<li>Acts as regularization during training.</li>
<li>Zero overhead at test-time: can be fused with conv!</li>
</ol>
<p><strong>Cons:</strong></p>
<p>Behaves differently during training and testing: this is a very common source of bugs!</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-all_norm.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-all_norm.png"/></a></p>
<h3 id="transfer-learning">Transfer Learning<a class="headerlink" href="#transfer-learning" title="Permanent link">¶</a></h3>
<p>Train on a pre-trained model with other datasets.</p>
<p><strong>An empirical suggestion:</strong></p>
<table>
<thead>
<tr>
<th></th>
<th><strong>very similar  dataset</strong></th>
<th><strong>very different  dataset</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>very little data</strong></td>
<td>Use Linear Classifier on top layer.</td>
<td>You’re in trouble… Try linear classifier from different stages.</td>
</tr>
<tr>
<td><strong>quite a lot of data</strong></td>
<td>Finetune a few layers.</td>
<td>Finetune a larger number of layers.</td>
</tr>
</tbody>
</table>
<h3 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">¶</a></h3>
<h4 id="common-pattern-of-regularization">Common Pattern of Regularization<a class="headerlink" href="#common-pattern-of-regularization" title="Permanent link">¶</a></h4>
<p>Training: Add some kind of randomness. <span class="arithmatex">\(y=f(x,z)\)</span></p>
<p>Testing: Average out randomness (sometimes approximate). <span class="arithmatex">\(y=f(x)=E_z\big[f(x,z)\big]=\int p(z)f(x,z)dz\\\)</span></p>
<h4 id="regularization-term">Regularization Term<a class="headerlink" href="#regularization-term" title="Permanent link">¶</a></h4>
<p>L2 regularization: <span class="arithmatex">\(R(W)=\sum_k\sum_lW_{k,l}^2\)</span> (weight decay)</p>
<p>L1 regularization: <span class="arithmatex">\(R(W)=\sum_k\sum_l|W_{k,l}|\)</span></p>
<p>Elastic net : <span class="arithmatex">\(R(W)=\sum_k\sum_l\big(\beta W_{k,l}^2+|W_{k,l}|\big)\)</span> (L1+L2)</p>
<h4 id="dropout">Dropout<a class="headerlink" href="#dropout" title="Permanent link">¶</a></h4>
<p>Training: Randomly set some neurons to 0 with a probability <span class="arithmatex">\(p\)</span> .</p>
<p>Testing: Each neuron multiplies by dropout probability <span class="arithmatex">\(p\)</span> . (scale the output back)</p>
<p><strong>More common:</strong> Scale the output with <span class="arithmatex">\(\frac{1}{p}\)</span> when training, keep the original output when testing.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-dropout_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-dropout_p.png"/></a></p>
<p><strong>Why dropout works:</strong></p>
<ol>
<li>Forces the network to have a redundant representation. Prevents co-adaptation of features.</li>
<li><strong>Another interpretation:</strong> Dropout is training a large ensemble of models (that share parameters).</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-dropout.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-dropout.png"/></a></p>
<h4 id="batch-normalization_1">Batch Normalization<a class="headerlink" href="#batch-normalization_1" title="Permanent link">¶</a></h4>
<p>See above.</p>
<h4 id="data-augmentation">Data Augmentation<a class="headerlink" href="#data-augmentation" title="Permanent link">¶</a></h4>
<ol>
<li>Horizontal Flips</li>
<li>Random Crops and Scales</li>
<li>Color Jitter</li>
<li>Rotation</li>
<li>Stretching</li>
<li>Shearing</li>
<li>Lens Distortions</li>
<li>...</li>
</ol>
<p>There also exists automatic data augmentation method using neural networks.</p>
<h4 id="other-methods-and-summary">Other Methods and Summary<a class="headerlink" href="#other-methods-and-summary" title="Permanent link">¶</a></h4>
<p><strong>DropConnect</strong>: Drop connections between neurons.</p>
<p><strong>Fractional Max Pooling:</strong> Use randomized pooling regions.</p>
<p><strong>Stochastic Depth</strong>: Skip some layers in the network.</p>
<p><strong>Cutout:</strong> Set random image regions to zero.</p>
<p><strong>Mixup:</strong> Train on random blends of images.</p>
<table>
<thead>
<tr>
<th>Regularization Method</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dropout</td>
<td>For large fully-connected layers.</td>
</tr>
<tr>
<td>Batch Normalization &amp; Data Augmentation</td>
<td>Almost always a good idea.</td>
</tr>
<tr>
<td>Cutout &amp; Mixup</td>
<td>For small classification datasets.</td>
</tr>
</tbody>
</table>
<h3 id="hyperparameter-tuning">Hyperparameter Tuning<a class="headerlink" href="#hyperparameter-tuning" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th>Most Common Hyperparameters</th>
<th>Less Sensitive Hyperparameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>learning rate<br/>learning rate decay schedule<br/>weight decay</td>
<td>setting of momentum<br/>...</td>
</tr>
</tbody>
</table>
<p><strong>Tips on hyperparameter tuning:</strong></p>
<ol>
<li>Prefer one validation fold to cross-validation.</li>
<li>Search for hyperparameters on log scale. (e.g. multiply the hyperparameter by a fixed number <span class="arithmatex">\(k\)</span> at each search)</li>
<li>Prefer <strong>random search</strong> to grid search.</li>
<li>Careful with best values on border.</li>
<li>Stage your search from coarse to fine.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-random_search.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/7-random_search.png"/></a></p>
<h4 id="implementation">Implementation<a class="headerlink" href="#implementation" title="Permanent link">¶</a></h4>
<p>Have a <strong>worker</strong> that continuously samples random hyperparameters and performs the optimization. During the training, the worker will keep track of the validation performance after every epoch, and writes a model checkpoint to a file.</p>
<p>Have a <strong>master</strong> that launches or kills workers across a computing cluster, and may additionally inspect the checkpoints written by workers and plot their training statistics.</p>
<h4 id="common-procedures">Common Procedures<a class="headerlink" href="#common-procedures" title="Permanent link">¶</a></h4>
<ol>
<li><strong>Check initial loss.</strong></li>
</ol>
<p>Turn off weight decay, sanity check loss at initialization <span class="arithmatex">\(\log(C)\)</span> for softmax with <span class="arithmatex">\(C\)</span> classes.</p>
<ol start="2">
<li><strong>Overfit a small sample. (important)</strong></li>
</ol>
<p>Try to train to 100% training accuracy on a small sample of training data.</p>
<p>Fiddle with architecture, learning rate, weight initialization.</p>
<ol start="3">
<li><strong>Find learning rate that makes loss go down.</strong></li>
</ol>
<p>Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop significantly within 100 iterations.</p>
<p>Good learning rates to try: <span class="arithmatex">\(0.1,0.01,0.001,0.0001,\dots\)</span></p>
<ol start="4">
<li><strong>Coarse grid, train for 1-5 epochs.</strong></li>
</ol>
<p>Choose a few values of learning rate and weight decay around what worked from Step 3, train a few models for 1-5 epochs.\</p>
<p>Good weight decay to try: <span class="arithmatex">\(0.0001,0.00001,0\)</span></p>
<ol start="5">
<li><strong>Refine grid, train longer.</strong></li>
</ol>
<p>Pick best models from Step 4, train them for longer (10-20 epochs) without learning rate decay.</p>
<ol start="6">
<li><strong>Look at loss and accuracy curves.</strong></li>
<li><strong>GOTO step 5.</strong></li>
</ol>
<h3 id="gradient-checks">Gradient Checks<a class="headerlink" href="#gradient-checks" title="Permanent link">¶</a></h3>
<p><a href="https://cs231n.github.io/neural-networks-3/#gradcheck">CS231n Convolutional Neural Networks for Visual Recognition</a></p>
<p>Compute analytical gradient manually using <span class="arithmatex">\(f_a'=\frac{\partial f(x)}{\partial x}=\frac{f(x-h)-f(x+h)}{2h}\\\)</span></p>
<p>Get relative error between numerical gradient <span class="arithmatex">\(f_n'\)</span> and analytical gradient <span class="arithmatex">\(f_a'\)</span> using <span class="arithmatex">\(E=\frac{|f_n'-f_a'|}{\max{|f_n'|,|f_a'|}}\\\)</span></p>
<table>
<thead>
<tr>
<th>Relative Error</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(E&gt;10^{-2}\)</span></td>
<td>Probably <span class="arithmatex">\(f_n'\)</span> is wrong.</td>
</tr>
<tr>
<td><span class="arithmatex">\(10^{-2}&gt;E&gt;10^{-4}\)</span></td>
<td>Not good, should check the gradient.</td>
</tr>
<tr>
<td><span class="arithmatex">\(10^{-4}&gt;E&gt;10^{-6}\)</span></td>
<td>Okay for objectives with kinks. (e.g. ReLU)<br/>Not good for objectives with no kink. (e.g. softmax, tanh)</td>
</tr>
<tr>
<td><span class="arithmatex">\(10^{-7}&gt;E\)</span></td>
<td>Good.</td>
</tr>
</tbody>
</table>
<p><strong>Tips on gradient checks:</strong></p>
<ol>
<li>Use double precision.</li>
<li>Use only few data points.</li>
<li>Careful about kinks in the objective. (e.g. <span class="arithmatex">\(x=0\)</span> for ReLU activation)</li>
<li>Careful with the step size <span class="arithmatex">\(h\)</span>.</li>
<li>Use gradient check after the loss starts to go down.</li>
<li>Remember to turn off anything that may affect the gradient. (e.g. <strong>regularization / dropout / augmentations</strong>)</li>
<li>Check only few dimensions for <strong>every parameter</strong>. (reduce time cost)</li>
</ol>
<h2 id="8-visualizing-and-understanding">8 - Visualizing and Understanding<a class="headerlink" href="#8-visualizing-and-understanding" title="Permanent link">¶</a></h2>
<h3 id="feature-visualization-and-inversion">Feature Visualization and Inversion<a class="headerlink" href="#feature-visualization-and-inversion" title="Permanent link">¶</a></h3>
<h4 id="visualizing-what-models-have-learned">Visualizing what models have learned<a class="headerlink" href="#visualizing-what-models-have-learned" title="Permanent link">¶</a></h4>
<table>
<thead>
<tr>
<th>Visualize Areas</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Filters</td>
<td>Visualize the raw weights of each convolution kernel. (better in the first layer)</td>
</tr>
<tr>
<td>Final Layer Features</td>
<td>Run dimensionality reduction for features in the last FC layer. (PCA, t-SNE...)</td>
</tr>
<tr>
<td>Activations</td>
<td>Visualize activated areas. (<a href="https://arxiv.org/abs/1506.06579">Understanding Neural Networks Through Deep Visualization</a>)</td>
</tr>
</tbody>
</table>
<h4 id="understanding-input-pixels">Understanding input pixels<a class="headerlink" href="#understanding-input-pixels" title="Permanent link">¶</a></h4>
<h5 id="maximally-activating-patches">Maximally Activating Patches<a class="headerlink" href="#maximally-activating-patches" title="Permanent link">¶</a></h5>
<ol>
<li>Pick a layer and a channel.</li>
<li>Run many images through the network, record values of the chosen channel.</li>
<li>Visualize image patches that correspond to maximal activation features.</li>
</ol>
<p>For example, we have a layer with shape <span class="arithmatex">\(128\times13\times13\)</span>. We pick the 17th channel from all 128 channels. Then we run many pictures through the network. During each run we can find a maximal activation feature among all the <span class="arithmatex">\(13\times13\)</span> features in channel 17. We then record the corresponding picture patch for each maximal activation feature. At last, we visualize all picture patches for each feature.</p>
<p>This will help us find the relationship between each maximal activation feature and its corresponding picture patches.</p>
<p>(each row of the following picture represents a feature)</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-activating_patches.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-activating_patches.png"/></a></p>
<h5 id="saliency-via-occlusion">Saliency via Occlusion<a class="headerlink" href="#saliency-via-occlusion" title="Permanent link">¶</a></h5>
<p>Mask part of the image before feeding to CNN, check how much predicted probabilities change.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_occlusion.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_occlusion.png"/></a></p>
<h5 id="saliency-via-backprop">Saliency via Backprop<a class="headerlink" href="#saliency-via-backprop" title="Permanent link">¶</a></h5>
<ol>
<li>Compute gradient of (unnormalized) class score with respect to image pixels.</li>
<li>Take absolute value and max over RGB channels to get <strong>saliency maps</strong>.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_backprop.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_backprop.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_backprop_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-saliency_via_backprop_p.png"/></a></p>
<h5 id="intermediate-features-via-guided-backprop">Intermediate Features via Guided Backprop<a class="headerlink" href="#intermediate-features-via-guided-backprop" title="Permanent link">¶</a></h5>
<ol>
<li>Pick a single intermediate neuron. (e.g. one feature in a <span class="arithmatex">\(128\times13\times13\)</span> feature map)</li>
<li>Compute gradient of neuron value with respect to image pixels.</li>
</ol>
<p><a href="https://arxiv.org/abs/1412.6806">Striving for Simplicity: The All Convolutional Net</a></p>
<p>Just like "Maximally Activating Patches", this could find the part of an image that a neuron responds to.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-guided_backprop.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-guided_backprop.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-guided_backprop_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-guided_backprop_p.png"/></a></p>
<h5 id="gradient-ascent">Gradient Ascent<a class="headerlink" href="#gradient-ascent" title="Permanent link">¶</a></h5>
<p>Generate a synthetic image that maximally activates a neuron.</p>
<ol>
<li>Initialize image <span class="arithmatex">\(I\)</span> to zeros.</li>
<li>Forward image to compute current scores <span class="arithmatex">\(S_c(I)\)</span> (for class <span class="arithmatex">\(c\)</span> before softmax).</li>
<li>Backprop to get gradient of neuron value with respect to image pixels.</li>
<li>Make a small update to the image.</li>
</ol>
<p>Objective: <span class="arithmatex">\(\max S_c(I)-\lambda\lVert I\lVert^2\)</span></p>
<p><a href="https://arxiv.org/abs/1312.6034">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-gradient_ascent.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-gradient_ascent.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-gradient_ascent_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-gradient_ascent_p.png"/></a></p>
<h3 id="adversarial-examples">Adversarial Examples<a class="headerlink" href="#adversarial-examples" title="Permanent link">¶</a></h3>
<p>Find an fooling image that can make the network misclassify correctly-classified images when it is added to the image.</p>
<ol>
<li>Start from an arbitrary image.</li>
<li>Pick an arbitrary class.</li>
<li>Modify the image to maximize the class.</li>
<li>Repeat until network is fooled.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-adversarial_examples.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-adversarial_examples.png"/></a></p>
<h3 id="deepdream-and-style-transfer">DeepDream and Style Transfer<a class="headerlink" href="#deepdream-and-style-transfer" title="Permanent link">¶</a></h3>
<h4 id="feature-inversion">Feature Inversion<a class="headerlink" href="#feature-inversion" title="Permanent link">¶</a></h4>
<p>Given a CNN feature vector <span class="arithmatex">\(\Phi_0\)</span> for an image, find a new image <span class="arithmatex">\(x\)</span> that:</p>
<ol>
<li>Features of new image <span class="arithmatex">\(\Phi(x)\)</span> matches the given feature vector <span class="arithmatex">\(\Phi_0\)</span>.</li>
<li>"looks natural”. (image prior regularization)</li>
</ol>
<p>Objective: <span class="arithmatex">\(\min \lVert\Phi(x)-\Phi_0\lVert+\lambda R(x)\)</span></p>
<p><a href="https://arxiv.org/abs/1412.0035">Understanding Deep Image Representations by Inverting Them</a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-feature_inversion.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-feature_inversion.png"/></a></p>
<h4 id="deepdream-amplify-existing-features">DeepDream: Amplify Existing Features<a class="headerlink" href="#deepdream-amplify-existing-features" title="Permanent link">¶</a></h4>
<p>Given an image, amplify the neuron activations at a layer to generate a new one.</p>
<ol>
<li>Forward: compute activations at chosen layer.</li>
<li>Set gradient of chosen layer equal to its activation.</li>
<li>Backward: Compute gradient on image.</li>
<li>Update image.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-deepdream.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-deepdream.png"/></a></p>
<h4 id="texture-synthesis">Texture Synthesis<a class="headerlink" href="#texture-synthesis" title="Permanent link">¶</a></h4>
<h5 id="nearest-neighbor">Nearest Neighbor<a class="headerlink" href="#nearest-neighbor" title="Permanent link">¶</a></h5>
<ol>
<li>Generate pixels one at a time in scanline order</li>
<li>Form neighborhood of already generated pixels, copy the nearest neighbor from input.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_nn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_nn.png"/></a></p>
<h5 id="neural-texture-synthesis">Neural Texture Synthesis<a class="headerlink" href="#neural-texture-synthesis" title="Permanent link">¶</a></h5>
<p><span>Gram Matrix:<span class="heti-spacing"> </span></span><a href="https://zhuanlan.zhihu.com/p/187345192">格拉姆矩阵（Gram matrix）详细解读</a></p>
<ol>
<li>Pretrain a CNN on ImageNet.</li>
<li>Run input texture forward through CNN, record activations on every layer.</li>
</ol>
<p>Layer <span class="arithmatex">\(i\)</span> gives feature map of shape <span class="arithmatex">\(C_i\times H_i\times W_i\)</span>.</p>
<ol start="3">
<li>At each layer compute the <strong>Gram matrix</strong> <span class="arithmatex">\(G_i\)</span> giving outer product of features.</li>
</ol>
<ul>
<li>Reshape feature map at layer <span class="arithmatex">\(i\)</span> to <span class="arithmatex">\(C_i\times H_iW_i\)</span>.</li>
<li>Compute the <strong>Gram matrix</strong> <span class="arithmatex">\(G_i\)</span> with shape <span class="arithmatex">\(C_i\times C_i\)</span>.</li>
</ul>
<ol start="4">
<li>Initialize generated image from random noise.</li>
<li>Pass generated image through CNN, compute <strong>Gram matrix</strong> <span class="arithmatex">\(\hat{G}_l\)</span> on each layer.</li>
<li>Compute loss: Weighted sum of L2 distance between <strong>Gram matrices</strong>.</li>
</ol>
<ul>
<li><span class="arithmatex">\(E_l=\frac{1}{aN_l^2M_l^2}\sum_{i,j}\Big(G_i^{(i,j)}-\hat{G}_i^{(i,j)}\Big)^2\\\)</span></li>
<li><span class="arithmatex">\(\mathcal{L}(\vec{x},\hat{\vec{x}})=\sum_{l=0}^L\omega_lE_l\\\)</span></li>
</ul>
<ol start="7">
<li>Backprop to get gradient on image.</li>
<li>Make gradient step on image.</li>
<li>GOTO 5.</li>
</ol>
<p><a href="https://arxiv.org/abs/1505.07376">Texture Synthesis Using Convolutional Neural Networks</a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_neural.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_neural.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_neural_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-texture_synthesis_neural_p.png"/></a></p>
<h4 id="style-transfer">Style Transfer<a class="headerlink" href="#style-transfer" title="Permanent link">¶</a></h4>
<h5 id="feature-gram-reconstruction">Feature + Gram Reconstruction<a class="headerlink" href="#feature-gram-reconstruction" title="Permanent link">¶</a></h5>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_p.png"/></a></p>
<p><strong>Problem:</strong> Style transfer requires many forward / backward passes. Very slow!</p>
<h5 id="fast-style-transfer">Fast Style Transfer<a class="headerlink" href="#fast-style-transfer" title="Permanent link">¶</a></h5>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_fast.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_fast.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_fast_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/8-style_transfer_fast_p.png"/></a></p>
<h2 id="9-object-detection-and-image-segmentation">9 - Object Detection and Image Segmentation<a class="headerlink" href="#9-object-detection-and-image-segmentation" title="Permanent link">¶</a></h2>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-tasks.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-tasks.png"/></a></p>
<h3 id="semantic-segmentation">Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permanent link">¶</a></h3>
<p><strong>Paired Training Data:</strong> For each training image, each pixel is labeled with a semantic category.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation.png"/></a></p>
<p><strong>Fully Convolutional Network:</strong> Design a network with only convolutional layers without downsampling operators to make predictions for pixels all at once!</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation_full_conv.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation_full_conv.png"/></a></p>
<p><strong>Problem:</strong> Convolutions at original image resolution will be very expensive...</p>
<p><strong>Solution:</strong> Design fully convolutional network with <strong>downsampling</strong> and <strong>upsampling</strong> inside it!</p>
<ul>
<li><strong>Downsampling:</strong> Pooling, strided convolution.</li>
<li><strong>Upsampling:</strong> Unpooling, transposed convolution.</li>
</ul>
<p><strong>Unpooling:</strong></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Nearest Neighbor</th>
<th style="text-align: center;">"Bed of Nails"</th>
<th style="text-align: center;">"Position Memory"</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_nn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_nn.png"/></a></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_bed_of_nails.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_bed_of_nails.png"/></a></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_memory.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-unpooling_memory.png"/></a></td>
</tr>
</tbody>
</table>
<p><strong>Transposed Convolution:</strong> (example size <span class="arithmatex">\(3\times3\)</span>, stride <span class="arithmatex">\(2\)</span>, pad <span class="arithmatex">\(1\)</span>)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Normal Convolution</th>
<th style="text-align: center;">Transposed Convolution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_normal.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_normal.png"/></a></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution.png"/></a></td>
</tr>
<tr>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_normal_m.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_normal_m.png"/></a></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_m.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-transposed_convolution_m.png"/></a></td>
</tr>
</tbody>
</table>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation_full_conv_down.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-sematic_segmetation_full_conv_down.png"/></a></p>
<h3 id="object-detection">Object Detection<a class="headerlink" href="#object-detection" title="Permanent link">¶</a></h3>
<h4 id="single-object">Single Object<a class="headerlink" href="#single-object" title="Permanent link">¶</a></h4>
<p>Classification + Localization. (classification + regression problem)</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-object_detection_single.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-object_detection_single.png"/></a></p>
<h4 id="multiple-object">Multiple Object<a class="headerlink" href="#multiple-object" title="Permanent link">¶</a></h4>
<h5 id="r-cnn">R-CNN<a class="headerlink" href="#r-cnn" title="Permanent link">¶</a></h5>
<p>Using selective search to find “blobby” image regions that are likely to contain objects.</p>
<ol>
<li>Find regions of interest (RoI) using selective search. (region proposal)</li>
<li>Forward each region through ConvNet.</li>
<li>Classify features with SVMs.</li>
</ol>
<p><strong>Problem:</strong> Very slow. Need to do 2000 independent forward passes for each image!</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-rcnn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-rcnn.png"/></a></p>
<h5 id="fast-r-cnn">Fast R-CNN<a class="headerlink" href="#fast-r-cnn" title="Permanent link">¶</a></h5>
<p>Pass the image through ConvNet before cropping. Crop the conv feature instead.</p>
<ol>
<li>Run whole image through ConvNet.</li>
<li>Find regions of interest (RoI) from conv features using selective search. (<strong>region proposal</strong>)</li>
<li>Classify RoIs using CNN.</li>
</ol>
<p><strong>Problem:</strong> Runtime is dominated by region proposals. (about <span class="arithmatex">\(90\%\)</span> time cost)</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-fast_rcnn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-fast_rcnn.png"/></a></p>
<h5 id="faster-r-cnn">Faster R-CNN<a class="headerlink" href="#faster-r-cnn" title="Permanent link">¶</a></h5>
<p>Insert Region Proposal Network (<strong>RPN</strong>) to predict proposals from features.</p>
<p>Otherwise same as Fast R-CNN: Crop features for each proposal, classify each one.</p>
<p><strong>Region Proposal Network (RPN) :</strong> Slide many fixed windows over ConvNet features.</p>
<ol>
<li>Treat each point in the feature map as the <strong>anchor</strong>. </li>
</ol>
<p>We have <span class="arithmatex">\(k\)</span> fixed windows (<strong>anchor boxes</strong>) of different size/scale centered with each anchor.</p>
<ol start="2">
<li>For each anchor box, predict whether it contains an object.</li>
</ol>
<p>For positive boxes, also predict a corrections to the ground-truth box.</p>
<ol start="3">
<li>Slide anchor over the feature map, get the <strong>“objectness” score</strong> for each box at each point.</li>
<li>Sort the “objectness” score, take top <span class="arithmatex">\(300\)</span> as the proposals.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-faster_rcnn_rpn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-faster_rcnn_rpn.png"/></a></p>
<p><strong>Faster R-CNN is a Two-stage object detector:</strong></p>
<ol>
<li>First stage: Run once per image</li>
</ol>
<p>Backbone network</p>
<p>Region proposal network</p>
<ol start="2">
<li>Second stage: Run once per region</li>
</ol>
<p>Crop features: RoI pool / align</p>
<p>Predict object class</p>
<p>Prediction bbox offset</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-faster_rcnn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-faster_rcnn.png"/></a></p>
<h5 id="single-stage-object-detectors-yolo">Single-Stage Object Detectors: YOLO<a class="headerlink" href="#single-stage-object-detectors-yolo" title="Permanent link">¶</a></h5>
<p><a href="https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a></p>
<ol>
<li>Divide image into grids. (example image grids shape <span class="arithmatex">\(7\times7\)</span>)</li>
<li>Set anchors in the middle of each grid.</li>
<li>For each grid:
   - Using <span class="arithmatex">\(B\)</span> anchor boxes to regress <span class="arithmatex">\(5\)</span> numbers: <span class="arithmatex">\(\text{dx, dy, dh, dw, confidence}\)</span>.
   - Predict scores for each of <span class="arithmatex">\(C\)</span> classes.</li>
<li>Finally the output is <span class="arithmatex">\(7\times7\times(5B+C)\)</span>.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-yolo.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-yolo.png"/></a></p>
<h3 id="instance-segmentation">Instance Segmentation<a class="headerlink" href="#instance-segmentation" title="Permanent link">¶</a></h3>
<p><strong>Mask R-CNN:</strong> Add a small mask network that operates on each RoI and predicts a <span class="arithmatex">\(28\times28\)</span> binary mask.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-mask_rcnn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-mask_rcnn.png"/></a></p>
<p>Mask R-CNN performs very good results!</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-mask_rcnn_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/9-mask_rcnn_p.png"/></a></p>
<h2 id="10-recurrent-neural-networks">10 - Recurrent Neural Networks<a class="headerlink" href="#10-recurrent-neural-networks" title="Permanent link">¶</a></h2>
<p>Supplement content added according to <a href="https://www.deeplearningbook.org/contents/rnn.html">Deep Learning Book - RNN</a>.</p>
<h3 id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)<a class="headerlink" href="#recurrent-neural-network-rnn" title="Permanent link">¶</a></h3>
<h4 id="motivation-sequence-processing">Motivation: Sequence Processing<a class="headerlink" href="#motivation-sequence-processing" title="Permanent link">¶</a></h4>
<table>
<thead>
<tr>
<th style="text-align: center;">One to One</th>
<th style="text-align: center;">One to Many</th>
<th style="text-align: center;">Many to One</th>
<th style="text-align: center;">Many to Many</th>
<th style="text-align: center;">Many to Many</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_11.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_11.png"/></a></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_1m.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_1m.png"/></a></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_m1.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_m1.png"/></a></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_mm.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_mm.png"/></a></td>
<td style="text-align: center;"><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_mm_2.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_seqnence_mm_2.png"/></a></td>
</tr>
<tr>
<td style="text-align: center;">Vanilla Neural Networks</td>
<td style="text-align: center;">Image Captioning</td>
<td style="text-align: center;">Action Prediction</td>
<td style="text-align: center;">Video Captioning</td>
<td style="text-align: center;">Video Classification on Frame Level</td>
</tr>
</tbody>
</table>
<h4 id="vanilla-rnn">Vanilla RNN<a class="headerlink" href="#vanilla-rnn" title="Permanent link">¶</a></h4>
<p><span class="arithmatex">\(x^{(t)}\)</span> : Input at time <span class="arithmatex">\(t\)</span>.</p>
<p><span class="arithmatex">\(h^{(t)}\)</span> : State at time <span class="arithmatex">\(t\)</span>.</p>
<p><span class="arithmatex">\(o^{(t)}\)</span> : Output at time <span class="arithmatex">\(t\)</span>​​.</p>
<p><span class="arithmatex">\(y^{(t)}\)</span> : Expected output at time <span class="arithmatex">\(t\)</span>.</p>
<h5 id="many-to-one">Many to One<a class="headerlink" href="#many-to-one" title="Permanent link">¶</a></h5>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_vanilla_m1.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_vanilla_m1.png"/></a></p>
<table>
<thead>
<tr>
<th>Calculation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>State Transition</td>
<td><span class="arithmatex">\(h^{(t)}=\tanh(Wh^{(t-1)}+Ux^{(t)}+b)\)</span></td>
</tr>
<tr>
<td>Output Calculation</td>
<td><span class="arithmatex">\(o^{(\tau)}=\text{sigmoid}\ \big(Vh^{(\tau)}+c\big)\)</span></td>
</tr>
</tbody>
</table>
<h5 id="many-to-many-type-2">Many to Many (type 2)<a class="headerlink" href="#many-to-many-type-2" title="Permanent link">¶</a></h5>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_vanilla_mm.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_vanilla_mm.png"/></a></p>
<table>
<thead>
<tr>
<th>Calculation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>State Transition</td>
<td><span class="arithmatex">\(h^{(t)}=\tanh(Wh^{(t-1)}+Ux^{(t)}+b)\)</span></td>
</tr>
<tr>
<td>Output Calculation</td>
<td><span class="arithmatex">\(o^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+c\big)\)</span></td>
</tr>
</tbody>
</table>
<h4 id="rnn-with-teacher-forcing">RNN with Teacher Forcing<a class="headerlink" href="#rnn-with-teacher-forcing" title="Permanent link">¶</a></h4>
<p>Update current state according to last-time <strong>output</strong> instead of last-time <strong>state</strong>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_tf.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_tf.png"/></a></p>
<table>
<thead>
<tr>
<th>Calculation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>State Transition</td>
<td><span class="arithmatex">\(h^{(t)}=\tanh(Wo^{(t-1)}+Ux^{(t)}+b)\)</span></td>
</tr>
<tr>
<td>Output Calculation</td>
<td><span class="arithmatex">\(o^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+c\big)\)</span></td>
</tr>
</tbody>
</table>
<h4 id="rnn-with-output-forwarding">RNN with "Output Forwarding"<a class="headerlink" href="#rnn-with-output-forwarding" title="Permanent link">¶</a></h4>
<p>We can also combine last-state <strong>output</strong> with this-state <strong>input</strong> together.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_output.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_output.png"/></a></p>
<table>
<thead>
<tr>
<th>Calculation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>State Transition (training)</td>
<td><span class="arithmatex">\(h^{(t)}=\tanh(Wh^{(t-1)}+Ux^{(t)}+Ry^{(t-1)}+b)\)</span></td>
</tr>
<tr>
<td>State Transition (testing)</td>
<td><span class="arithmatex">\(h^{(t)}=\tanh(Wh^{(t-1)}+Ux^{(t)}+Ro^{(t-1)}+b)\)</span></td>
</tr>
<tr>
<td>Output Calculation</td>
<td><span class="arithmatex">\(o^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+c\big)\)</span></td>
</tr>
</tbody>
</table>
<p>Usually we use <span class="arithmatex">\(o^{(t-1)}\)</span> in place of <span class="arithmatex">\(y^{(t-1)}\)</span> at testing time.</p>
<h4 id="bidirectional-rnn">Bidirectional RNN<a class="headerlink" href="#bidirectional-rnn" title="Permanent link">¶</a></h4>
<p>When dealing with <strong>a whole input sequence</strong>, we can process features from two directions.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_bidirectional.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_bidirectional.png"/></a></p>
<table>
<thead>
<tr>
<th>Calculation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>State Transition (forward)</td>
<td><span class="arithmatex">\(h^{(t)}=\tanh(W_1h^{(t-1)}+U_1x^{(t)}+b_1)\)</span></td>
</tr>
<tr>
<td>State Transition (backward)</td>
<td><span class="arithmatex">\(g^{(t)}=\tanh(W_2g^{(t+1)}+U_2x^{(t)}+b_2)\)</span></td>
</tr>
<tr>
<td>Output Calculation</td>
<td><span class="arithmatex">\(o^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+Wg^{(t)}+c\big)\)</span></td>
</tr>
</tbody>
</table>
<h4 id="encoder-decoder-sequence-to-sequence-rnn">Encoder-Decoder Sequence to Sequence RNN<a class="headerlink" href="#encoder-decoder-sequence-to-sequence-rnn" title="Permanent link">¶</a></h4>
<p>This is a <strong>many-to-many structure (type 1)</strong>.</p>
<p>First we encode information according to <span class="arithmatex">\(x\)</span> with no output.</p>
<p>Later we decode information according to <span class="arithmatex">\(y\)</span> with no input.</p>
<p><span class="arithmatex">\(C\)</span> : Context vector, often <span class="arithmatex">\(C=h^{(T)}\)</span> (last state of encoder).</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_encoder.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_structure_encoder.png"/></a></p>
<table>
<thead>
<tr>
<th>Calculation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>State Transition (encode)</td>
<td><span class="arithmatex">\(h^{(t)}=\tanh(W_1h^{(t-1)}+U_1x^{(t)}+b_1)\)</span></td>
</tr>
<tr>
<td>State Transition (decode, training)</td>
<td><span class="arithmatex">\(s^{(t)}=\tanh(W_2s^{(t-1)}+U_2y^{(t)}+TC+b_2)\)</span></td>
</tr>
<tr>
<td>State Transition (decode, testing)</td>
<td><span class="arithmatex">\(s^{(t)}=\tanh(W_2s^{(t-1)}+U_2o^{(t)}+TC+b_2)\)</span></td>
</tr>
<tr>
<td>Output Calculation</td>
<td><span class="arithmatex">\(o^{(t)}=\text{sigmoid}\ \big(Vs^{(t)}+c\big)\)</span></td>
</tr>
</tbody>
</table>
<h4 id="example-image-captioning">Example: Image Captioning<a class="headerlink" href="#example-image-captioning" title="Permanent link">¶</a></h4>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_example.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-rnn_example.png"/></a></p>
<h4 id="summary_2">Summary<a class="headerlink" href="#summary_2" title="Permanent link">¶</a></h4>
<p><strong>Advantages of RNN:</strong></p>
<ol>
<li>Can process any length input.</li>
<li>Computation for step <span class="arithmatex">\(t\)</span> can (in theory) use information from many steps back.</li>
<li>Model size doesn’t increase for longer input.</li>
<li>Same weights applied on every timestep, so there is symmetry in how inputs are processed.</li>
</ol>
<p><strong>Disadvantages of RNN:</strong></p>
<ol>
<li>Recurrent computation is slow.</li>
<li>In practice, difficult to access information from many steps back.</li>
<li>Problems with gradient exploding and gradient vanishing. <strong>(check <a href="https://www.deeplearningbook.org/contents/rnn.html">Deep Learning Book - RNN</a> Page 396, Chap 10.7)</strong></li>
</ol>
<h3 id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)<a class="headerlink" href="#long-short-term-memory-lstm" title="Permanent link">¶</a></h3>
<p>Add a "cell block" to store history weights.</p>
<p><span class="arithmatex">\(c^{(t)}\)</span> : Cell at time <span class="arithmatex">\(t\)</span>.</p>
<p><span class="arithmatex">\(f^{(t)}\)</span> : <strong>Forget gate</strong> at time <span class="arithmatex">\(t\)</span>. Deciding whether to erase the cell.</p>
<p><span class="arithmatex">\(i^{(t)}\)</span> : <strong>Input gate</strong> at time <span class="arithmatex">\(t\)</span>. Deciding whether to write to the cell.</p>
<p><span class="arithmatex">\(g^{(t)}\)</span> : <strong>External input gate</strong> at time <span class="arithmatex">\(t\)</span>. Deciding how much to write to the cell.</p>
<p><span class="arithmatex">\(o^{(t)}\)</span> : <strong>Output gate</strong> at time <span class="arithmatex">\(t\)</span>. Deciding how much to reveal the cell.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-lstm.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-lstm.png"/></a></p>
<table>
<thead>
<tr>
<th>Calculation (Gate)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Forget Gate</td>
<td><span class="arithmatex">\(f^{(t)}=\text{sigmoid}\ \big(W_fh^{(t-1)}+U_fx^{(t)}+b_f\big)\)</span></td>
</tr>
<tr>
<td>Input Gate</td>
<td><span class="arithmatex">\(i^{(t)}=\text{sigmoid}\ \big(W_ih^{(t-1)}+U_ix^{(t)}+b_i\big)\)</span></td>
</tr>
<tr>
<td>External Input Gate</td>
<td><span class="arithmatex">\(g^{(t)}=\tanh(W_gh^{(t-1)}+U_gx^{(t)}+b_g)\)</span></td>
</tr>
<tr>
<td>Output Gate</td>
<td><span class="arithmatex">\(o^{(t)}=\text{sigmoid}\ \big(W_oh^{(t-1)}+U_ox^{(t)}+b_o\big)\)</span></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Calculation (Main)</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Cell Transition</td>
<td><span class="arithmatex">\(c^{(t)}=f^{(t)}\odot c^{(t-1)}+i^{(t)}\odot g^{(t)}\)</span></td>
</tr>
<tr>
<td>State Transition</td>
<td><span class="arithmatex">\(h^{(t)}=o^{(t)}\odot\tanh(c^{(t)})\)</span></td>
</tr>
<tr>
<td>Output Calculation</td>
<td><span class="arithmatex">\(O^{(t)}=\text{sigmoid}\ \big(Vh^{(t)}+c\big)\)</span></td>
</tr>
</tbody>
</table>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-lstm_gradient.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/10-lstm_gradient.png"/></a></p>
<h3 id="other-rnn-variants">Other RNN Variants<a class="headerlink" href="#other-rnn-variants" title="Permanent link">¶</a></h3>
<p>GRU...</p>
<h2 id="11-attention-and-transformers">11 - Attention and Transformers<a class="headerlink" href="#11-attention-and-transformers" title="Permanent link">¶</a></h2>
<h3 id="rnn-with-attention">RNN with Attention<a class="headerlink" href="#rnn-with-attention" title="Permanent link">¶</a></h3>
<p><strong>Encoder-Decoder Sequence to Sequence RNN Problem:</strong></p>
<p>Input sequence bottlenecked through a fixed-sized context vector <span class="arithmatex">\(C\)</span>. (e.g. <span class="arithmatex">\(T=1000\)</span>)</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_sequence.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_sequence.png"/></a></p>
<p><strong>Intuitive Solution:</strong></p>
<p>Generate new context vector <span class="arithmatex">\(C_t\)</span> at each step <span class="arithmatex">\(t\)</span> !</p>
<p><span class="arithmatex">\(e_{t,i}\)</span> : Alignment score for input <span class="arithmatex">\(i\)</span> at state <span class="arithmatex">\(t\)</span>. <strong>(scalar)</strong></p>
<p><span class="arithmatex">\(a_{t,i}\)</span> : Attention weight for input <span class="arithmatex">\(i\)</span> at state <span class="arithmatex">\(t\)</span>.</p>
<p><span class="arithmatex">\(C_t\)</span> : Context vector at state <span class="arithmatex">\(t\)</span>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_1.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_1.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_2.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_2.png"/></a></p>
<table>
<thead>
<tr>
<th>Calculation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Alignment Score</td>
<td><span class="arithmatex">\(e_i^{(t)}=f(s^{(t-1)},h^{(i)})\)</span>.<br/>Where <span class="arithmatex">\(f\)</span> is an MLP.</td>
</tr>
<tr>
<td>Attention Weight</td>
<td><span class="arithmatex">\(a_i^{(t)}=\text{softmax}\ (e_i^{(t)})\)</span>.<br/>Softmax includes all <span class="arithmatex">\(e_i\)</span> at state <span class="arithmatex">\(t\)</span>.</td>
</tr>
<tr>
<td>Context Vector</td>
<td><span class="arithmatex">\(C^{(t)}=\sum_i a_i^{(t)}h^{(i)}\)</span></td>
</tr>
<tr>
<td>Decoder State Transition</td>
<td><span class="arithmatex">\(s^{(t)}=\tanh(Ws^{(t-1)}+Uy^{(t)}+TC^{(t)}+b)\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Example on Image Captioning:</strong></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_example.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_example.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_example_2.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-rnn_attention_example_2.png"/></a></p>
<h3 id="general-attention-layer">General Attention Layer<a class="headerlink" href="#general-attention-layer" title="Permanent link">¶</a></h3>
<p>Add linear transformations to the input vector before attention.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-general_attention.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-general_attention.png"/></a></p>
<p><strong>Notice:</strong></p>
<ol>
<li>Number of queries <span class="arithmatex">\(q\)</span> is variant. (can be <strong>different</strong> from the number of keys <span class="arithmatex">\(k\)</span>)</li>
<li>Number of outputs <span class="arithmatex">\(y\)</span> is equal to the number of queries <span class="arithmatex">\(q\)</span>.</li>
</ol>
<p>Each <span class="arithmatex">\(y\)</span> is a linear weighting of values <span class="arithmatex">\(v\)</span>.</p>
<ol start="3">
<li>Alignment <span class="arithmatex">\(e\)</span> is divided by <span class="arithmatex">\(\sqrt{D}\)</span> to avoid "explosion of softmax", where <span class="arithmatex">\(D\)</span> is the dimension of input feature.</li>
</ol>
<h3 id="self-attention-layer">Self-attention Layer<a class="headerlink" href="#self-attention-layer" title="Permanent link">¶</a></h3>
<p>The query vectors <span class="arithmatex">\(q\)</span> are also generated from the inputs.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention.png"/></a></p>
<p>In this way, the shape of <span class="arithmatex">\(y\)</span> is equal to the shape of <span class="arithmatex">\(x\)</span>.</p>
<p><strong>Example with CNN:</strong></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_example.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_example.png"/></a></p>
<h3 id="positional-encoding">Positional Encoding<a class="headerlink" href="#positional-encoding" title="Permanent link">¶</a></h3>
<p>Self-attention layer doesn’t care about the orders of the inputs!</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_problem.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_problem.png"/></a></p>
<p>To encode ordered sequences like language or spatially ordered image features, we can add positional encoding to the inputs.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding.png"/></a></p>
<p>We use a function <span class="arithmatex">\(P:R\rightarrow R^d\)</span> to process the <strong>position</strong> <span class="arithmatex">\(i\)</span> into a <strong>d-dimensional vector</strong> <span class="arithmatex">\(p_i=P(i)\)</span>.</p>
<table>
<thead>
<tr>
<th>Constraint Condition of <span class="arithmatex">\(P\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Uniqueness</td>
<td><span class="arithmatex">\(P(i)\ne P(j)\)</span></td>
</tr>
<tr>
<td>Equidistance</td>
<td><span class="arithmatex">\(\lVert P(i+k)-P(i)\rVert^2=\lVert P(j+k)-P(j)\rVert^2\)</span></td>
</tr>
<tr>
<td>Boundness</td>
<td><span class="arithmatex">\(P(i)\in[a,b]\)</span></td>
</tr>
<tr>
<td>Determinacy</td>
<td><span class="arithmatex">\(P(i)\)</span> is always a static value. (function is not dynamic)</td>
</tr>
</tbody>
</table>
<p>We can either train a encoder model, or design a fixed function.</p>
<p><strong>A Practical Positional Encoding Method:</strong> Using <span class="arithmatex">\(\sin\)</span> and <span class="arithmatex">\(\cos\)</span> with different frequency <span class="arithmatex">\(\omega\)</span> at different dimension.</p>
<p><span class="arithmatex">\(P(t)=\begin{bmatrix}\sin(\omega_1,t)\\\cos(\omega_1,t)\\\\\sin(\omega_2,t)\\\cos(\omega_2,t)\\\vdots\\\sin(\omega_{\frac{d}{2}},t)\\\cos(\omega_{\frac{d}{2}},t)\end{bmatrix}\)</span>, where frequency <span class="arithmatex">\(\omega_k=\frac{1}{10000^{\frac{2k}{d}}}\\\)</span>. (wave length <span class="arithmatex">\(\lambda=\frac{1}{\omega}=10000^{\frac{2k}{d}}\\\)</span>)</p>
<p><span class="arithmatex">\(P(t)=\begin{bmatrix}\sin(1/10000^{\frac{2}{d}},t)\\\cos(1/10000^{\frac{2}{d}},t)\\\\\sin(1/10000^{\frac{4}{d}},t)\\\cos(1/10000^{\frac{4}{d}},t)\\\vdots\\\sin(1/10000^1,t)\\\cos(1/10000^1,t)\end{bmatrix}\)</span>, after we substitute <span class="arithmatex">\(\omega_k\)</span> into the equation.</p>
<p><span class="arithmatex">\(P(t)\)</span> is a vector with size <span class="arithmatex">\(d\)</span>, where <span class="arithmatex">\(d\)</span> is a hyperparameter to choose according to the length of input sequence.</p>
<p>An intuition of this method is the binary encoding of numbers.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding_intuition.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding_intuition.png"/></a></p>
<p><a href="https://www.bilibili.com/video/BV1E3411B7Bz"><span>[lecture 11d]<span class="heti-spacing"> </span></span>注意力和<span class="heti-skip"><span class="heti-spacing"> </span>transformer (positional encoding<span class="heti-spacing"> </span></span>补充，代码实现，距离计算<span><span class="heti-spacing"> </span>)</span></a></p>
<p><strong>It is easy to prove that <span class="arithmatex">\(P(t)\)</span> satisfies "Equidistance":</strong> (set <span class="arithmatex">\(d=2\)</span> for example)</p>
<p><span class="arithmatex">\(\begin{aligned}\lVert P(i+k)-P(i)\rVert^2&amp;=\big[\sin(\omega_1,i+k)-\sin(\omega_1,i)\big]^2+\big[\cos(\omega_1,i+k)-\cos(\omega_1,i)\big]^2\\&amp;=2-2\sin(\omega_1,i+k)\sin(\omega_1,i)-2\cos(\omega_1,i+k)\cos(\omega_1,i)\\&amp;=2-2\cos(\omega_1,k)\end{aligned}\)</span></p>
<p>So the distance is not associated with <span class="arithmatex">\(i\)</span>, we have <span class="arithmatex">\(\lVert P(i+k)-P(i)\rVert^2=\lVert P(j+k)-P(j)\rVert^2\)</span>.</p>
<p><strong>Visualization of <span class="arithmatex">\(P(t)\)</span> features:</strong> (set <span class="arithmatex">\(d=32\)</span>, <span class="arithmatex">\(x\)</span> axis represents the position of sequence)</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding_p.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-self_attention_positional_encoding_p.png"/></a></p>
<h3 id="masked-self-attention-layer">Masked Self-attention Layer<a class="headerlink" href="#masked-self-attention-layer" title="Permanent link">¶</a></h3>
<p>To prevent vectors from looking at future vectors, we manually set alignment scores to <span class="arithmatex">\(-\infty\)</span>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-masked_self_attention.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-masked_self_attention.png"/></a></p>
<h3 id="multi-head-self-attention-layer">Multi-head Self-attention Layer<a class="headerlink" href="#multi-head-self-attention-layer" title="Permanent link">¶</a></h3>
<p>Multiple self-attention heads in parallel.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-multihead_self_attention.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-multihead_self_attention.png"/></a></p>
<h3 id="transformer">Transformer<a class="headerlink" href="#transformer" title="Permanent link">¶</a></h3>
<p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>
<h4 id="encoder-block">Encoder Block<a class="headerlink" href="#encoder-block" title="Permanent link">¶</a></h4>
<p><strong>Inputs:</strong> Set of vectors <span class="arithmatex">\(z\)</span>. (in which <span class="arithmatex">\(z_i\)</span> can be a <strong>word</strong> in a sentence, or a <strong>pixel</strong> in a picture...)</p>
<p><strong>Output:</strong> Set of context vectors <span class="arithmatex">\(c\)</span>. (encoded <strong>features</strong> of <span class="arithmatex">\(z\)</span>)</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_encoder.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_encoder.png"/></a></p>
<p>The number of blocks <span class="arithmatex">\(N=6\)</span> in original paper.</p>
<p><strong>Notice:</strong></p>
<ol>
<li>Self-attention is the only interaction <strong>between vectors</strong> <span class="arithmatex">\(x_0,x_1,\dots,x_n\)</span>.</li>
<li>Layer norm and MLP operate independently <strong>per vector</strong>.</li>
<li>Highly scalable, highly parallelizable, but high memory usage.</li>
</ol>
<h4 id="decoder-block">Decoder Block<a class="headerlink" href="#decoder-block" title="Permanent link">¶</a></h4>
<p><strong>Inputs:</strong> Set of vectors <span class="arithmatex">\(y\)</span>. (<span class="arithmatex">\(y_i\)</span> can be a <strong>word</strong> in a sentence, or a <strong>pixel</strong> in a picture...)</p>
<p><strong>Inputs:</strong> Set of context vectors <span class="arithmatex">\(c\)</span>.</p>
<p><strong>Output:</strong> Set of vectors <span class="arithmatex">\(y'\)</span>. (decoded result, <span class="arithmatex">\(y'_i=y_{i+1}\)</span> for the first <span class="arithmatex">\(n-1\)</span> number of <span class="arithmatex">\(y'\)</span>)</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_decoder.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_decoder.png"/></a></p>
<p>The number of blocks <span class="arithmatex">\(N=6\)</span> in original paper.</p>
<p><strong>Notice:</strong></p>
<ol>
<li>Masked self-attention only interacts with <strong>past inputs</strong>.</li>
<li>Multi-head attention block is <strong>NOT</strong> self-attention. It attends over encoder outputs.</li>
<li>Highly scalable, highly parallelizable, but high memory usage. (same as encoder)</li>
</ol>
<p><strong>Why we need mask in decoder:</strong></p>
<ol>
<li>Needs for the special formation of output <span class="arithmatex">\(y'_i=y_{i+1}\)</span>.</li>
<li>Needs for parallel computation.</li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/166608727">举个例子讲下<span class="heti-skip"><span class="heti-spacing"> </span>transformer<span class="heti-spacing"> </span></span>的输入输出细节及其他</a></p>
<p><a href="https://blog.csdn.net/season77us/article/details/104144613">在测试或者预测时，<span>Transformer<span class="heti-spacing"> </span></span>里<span class="heti-skip"><span class="heti-spacing"> </span>decoder<span class="heti-spacing"> </span></span>为什么还需要<span><span class="heti-spacing"> </span>seq mask</span>？</a></p>
<h4 id="example-on-image-captioning-only-with-transformers">Example on Image Captioning (Only with Transformers)<a class="headerlink" href="#example-on-image-captioning-only-with-transformers" title="Permanent link">¶</a></h4>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_example.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_example.png"/></a></p>
<h3 id="comparing-rnns-to-transformer">Comparing RNNs to Transformer<a class="headerlink" href="#comparing-rnns-to-transformer" title="Permanent link">¶</a></h3>
<table>
<thead>
<tr>
<th></th>
<th>RNNs</th>
<th>Transformer</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Pros</strong></td>
<td>LSTMs work reasonably well for <strong>long sequences</strong>.</td>
<td>1. Good at <strong>long sequences</strong>. Each attention calculation looks at all inputs.<br/>2. Can operate over unordered sets or <strong>ordered sequences</strong> with positional encodings.<br/>3. <strong>Parallel computation:</strong> All alignment and attention scores for all inputs can be done in parallel.</td>
</tr>
<tr>
<td><strong>Cons</strong></td>
<td>1. Expects an <strong>ordered sequences</strong> of inputs.<br/>2. <strong>Sequential computation:</strong> Subsequent hidden states can only be computed after the previous ones are done.</td>
<td><strong>Requires a lot of memory:</strong> <span class="arithmatex">\(N\times M\)</span> alignment and attention scalers need to be calculated and stored for a single self-attention head.</td>
</tr>
</tbody>
</table>
<h3 id="comparing-convnets-to-transformer">Comparing ConvNets to Transformer<a class="headerlink" href="#comparing-convnets-to-transformer" title="Permanent link">¶</a></h3>
<p>ConvNets strike back!</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_compare.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/11-transformer_compare.png"/></a></p>
<h2 id="12-video-understanding">12 - Video Understanding<a class="headerlink" href="#12-video-understanding" title="Permanent link">¶</a></h2>
<h3 id="video-classification">Video Classification<a class="headerlink" href="#video-classification" title="Permanent link">¶</a></h3>
<p>Take video classification task for example.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_classification.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_classification.png"/></a></p>
<p>Input size: <span class="arithmatex">\(C\times T\times H\times W\)</span>.</p>
<p>The problem is, videos are quite big. We can't afford to train on raw videos, instead we train on video clips.</p>
<table>
<thead>
<tr>
<th>Raw Videos</th>
<th>Video Clips</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(1920\times1080,\ 30\text{fps}\)</span></td>
<td><span class="arithmatex">\(112\times112,\ 5\text{f}/3.2\text{s}\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(10\text{GB}/\text{min}\)</span></td>
<td><span class="arithmatex">\(588\text{KB}/\text{min}\)</span></td>
</tr>
</tbody>
</table>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_classification_clips.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_classification_clips.png"/></a></p>
<h3 id="plain-cnn-structure">Plain CNN Structure<a class="headerlink" href="#plain-cnn-structure" title="Permanent link">¶</a></h3>
<h4 id="single-frame-2d-cnn">Single Frame 2D-CNN<a class="headerlink" href="#single-frame-2d-cnn" title="Permanent link">¶</a></h4>
<p>Train a normal 2D-CNN model.</p>
<p>Classify each frame independently.</p>
<p>Average the result of each frame as the final result.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-single_frame_cnn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-single_frame_cnn.png"/></a></p>
<h4 id="late-fusion">Late Fusion<a class="headerlink" href="#late-fusion" title="Permanent link">¶</a></h4>
<p>Get high-level appearance of each frame, and combine them.</p>
<p>Run 2D-CNN on each frame, pool features and feed to Linear Layers.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-late_fusion.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-late_fusion.png"/></a></p>
<p><strong>Problem:</strong> Hard to compare low-level motion between frames.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-late_fusion_problem.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-late_fusion_problem.png"/></a></p>
<h4 id="early-fusion">Early Fusion<a class="headerlink" href="#early-fusion" title="Permanent link">¶</a></h4>
<p>Compare frames with very first Conv Layer, after that normal 2D-CNN.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-early_fusion.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-early_fusion.png"/></a></p>
<p><strong>Problem:</strong> One layer of temporal processing may not be enough!</p>
<h4 id="3d-cnn">3D-CNN<a class="headerlink" href="#3d-cnn" title="Permanent link">¶</a></h4>
<p><strong>Convolve on 3 dimensions:</strong> Height, Width, Time.</p>
<p><strong>Input size:</strong> <span class="arithmatex">\(C_{in}\times T\times H\times W\)</span>.</p>
<p><strong>Kernel size:</strong> <span class="arithmatex">\(C_{in}\times C_{out}\times 3\times 3\times 3\)</span>.</p>
<p><strong>Output size:</strong> <span class="arithmatex">\(C_{out}\times T\times H\times W\)</span>. (with zero paddling)</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-3d_cnn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-3d_cnn.png"/></a></p>
<h4 id="c3d-vgg-of-3d-cnns">C3D (VGG of 3D-CNNs)<a class="headerlink" href="#c3d-vgg-of-3d-cnns" title="Permanent link">¶</a></h4>
<p>The cost is quite expensive...</p>
<table>
<thead>
<tr>
<th>Network</th>
<th>Calculation</th>
</tr>
</thead>
<tbody>
<tr>
<td>AlexNet</td>
<td>0.7 GFLOP</td>
</tr>
<tr>
<td>VGG-16</td>
<td>13.6 GFLOP</td>
</tr>
<tr>
<td>C3D</td>
<td><strong>39.5</strong> GFLOP</td>
</tr>
</tbody>
</table>
<h4 id="two-stream-networks">Two-Stream Networks<a class="headerlink" href="#two-stream-networks" title="Permanent link">¶</a></h4>
<p>Separate motion and appearance.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-two_stream_flow.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-two_stream_flow.png"/></a></p>
<h4 id="i3d-inflating-2d-networks-to-3d">I3D (Inflating 2D Networks to 3D)<a class="headerlink" href="#i3d-inflating-2d-networks-to-3d" title="Permanent link">¶</a></h4>
<p>Take a 2D-CNN architecture.</p>
<p>Replace each 2D conv/pool layer with a 3D version.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-i3d.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-i3d.png"/></a></p>
<h3 id="modeling-long-term-temporal-structure">Modeling Long-term Temporal Structure<a class="headerlink" href="#modeling-long-term-temporal-structure" title="Permanent link">¶</a></h3>
<h4 id="recurrent-convolutional-network">Recurrent Convolutional Network<a class="headerlink" href="#recurrent-convolutional-network" title="Permanent link">¶</a></h4>
<p>Similar to multi-layer RNN, we replace the <strong>dot-product</strong> operation with <strong>convolution</strong>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-rcn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-rcn.png"/></a></p>
<p>Feature size in layer <span class="arithmatex">\(L\)</span>, time <span class="arithmatex">\(t-1\)</span>: <span class="arithmatex">\(W_h\times H\times W\)</span>.</p>
<p>Feature size in layer <span class="arithmatex">\(L-1\)</span>, time <span class="arithmatex">\(t\)</span>: <span class="arithmatex">\(W_x\times H\times W\)</span>.</p>
<p>Feature size in layer <span class="arithmatex">\(L\)</span>, time <span class="arithmatex">\(t\)</span>: <span class="arithmatex">\((W_h+W_x)\times H\times W\)</span>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-rcn_inside.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-rcn_inside.png"/></a></p>
<p><strong>Problem:</strong> RNNs are slow for long sequences. (can’t be parallelized)</p>
<h4 id="spatio-temporal-self-attention">Spatio-temporal Self-attention<a class="headerlink" href="#spatio-temporal-self-attention" title="Permanent link">¶</a></h4>
<p>Introduce self-attention into video classification problems.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-self_attention.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-self_attention.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-self_attention_net.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-self_attention_net.png"/></a></p>
<h4 id="vision-transformers-for-video">Vision Transformers for Video<a class="headerlink" href="#vision-transformers-for-video" title="Permanent link">¶</a></h4>
<p>Factorized attention: Attend over space / time.</p>
<p>So many papers...</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-vision_transformer.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-vision_transformer.png"/></a></p>
<h3 id="visualizing-video-models">Visualizing Video Models<a class="headerlink" href="#visualizing-video-models" title="Permanent link">¶</a></h3>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_visualizing.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_visualizing.png"/></a></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_visualizing_2.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-video_visualizing_2.png"/></a></p>
<h3 id="multimodal-video-understanding">Multimodal Video Understanding<a class="headerlink" href="#multimodal-video-understanding" title="Permanent link">¶</a></h3>
<h4 id="temporal-action-localization">Temporal Action Localization<a class="headerlink" href="#temporal-action-localization" title="Permanent link">¶</a></h4>
<p>Given a long untrimmed video sequence, identify frames corresponding to different actions.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_temporal_localization.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_temporal_localization.png"/></a></p>
<h4 id="spatio-temporal-detection">Spatio-Temporal Detection<a class="headerlink" href="#spatio-temporal-detection" title="Permanent link">¶</a></h4>
<p>Given a long untrimmed video, detect all the people in both space and time and classify the activities they are performing.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_s_t_detection.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_s_t_detection.png"/></a></p>
<h4 id="visually-guided-audio-source-separation">Visually-guided Audio Source Separation<a class="headerlink" href="#visually-guided-audio-source-separation" title="Permanent link">¶</a></h4>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_voice_separation.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/12-multimodal_voice_separation.png"/></a></p>
<p>And So on...</p>
<h2 id="13-generative-models">13 - Generative Models<a class="headerlink" href="#13-generative-models" title="Permanent link">¶</a></h2>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-generative_model.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-generative_model.png"/></a></p>
<h3 id="pixelrnn-and-pixelcnn">PixelRNN and PixelCNN<a class="headerlink" href="#pixelrnn-and-pixelcnn" title="Permanent link">¶</a></h3>
<h4 id="fully-visible-belief-network-fvbn">Fully Visible Belief Network (FVBN)<a class="headerlink" href="#fully-visible-belief-network-fvbn" title="Permanent link">¶</a></h4>
<p><span class="arithmatex">\(p(x)\)</span> : Likelihood of image <span class="arithmatex">\(x\)</span>.</p>
<p><span class="arithmatex">\(p(x_1,x_2,\dots,x_n)\)</span> : Joint likelihood of all <span class="arithmatex">\(n\)</span> pixels in image <span class="arithmatex">\(x\)</span>.</p>
<p><span class="arithmatex">\(p(x_i|x_1,x_2,\dots,x_{i-1})\)</span> : Probability of pixel <span class="arithmatex">\(i\)</span> value given all previous pixels.</p>
<p>For explicit density models, we have <span class="arithmatex">\(p(x)=p(x_1,x_2,\dots,x_n)=\prod_{i=1}^np(x_i|x_1,x_2,\dots,x_{i-1})\\\)</span>.</p>
<p><strong>Objective:</strong> Maximize the likelihood of training data.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-likelihood.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-likelihood.png"/></a></p>
<h4 id="pixelrnn">PixelRNN<a class="headerlink" href="#pixelrnn" title="Permanent link">¶</a></h4>
<p>Generate image pixels starting from corner.</p>
<p>Dependency on previous pixels modeled using an RNN (LSTM).</p>
<p><strong>Drawback:</strong> Sequential generation is slow in both training and inference!</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-pixel_rnn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-pixel_rnn.png"/></a></p>
<h4 id="pixelcnn">PixelCNN<a class="headerlink" href="#pixelcnn" title="Permanent link">¶</a></h4>
<p>Still generate image pixels starting from corner.</p>
<p>Dependency on previous pixels modeled using a CNN over context region (masked convolution).</p>
<p><strong>Drawback:</strong> Though its training is faster, its generation is still slow. <strong>(pixel by pixel)</strong></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-pixel_cnn.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-pixel_cnn.png"/></a></p>
<h3 id="variational-autoencoder">Variational Autoencoder<a class="headerlink" href="#variational-autoencoder" title="Permanent link">¶</a></h3>
<p>Supplement content added according to <a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a>. (<strong>paper with notes:</strong> <a href="..\Variational Autoencoder\papes\VAE Tutorial.pdf">VAE Tutorial.pdf</a>)</p>
<p><a href="https://zhuanlan.zhihu.com/p/34998569">变分自编码器<span><span class="heti-spacing"> </span>VAE</span>：原来是这么一回事<span class="heti-skip"><span class="heti-spacing"> </span>|<span class="heti-spacing"> </span></span>附开源代码</a></p>
<h4 id="autoencoder">Autoencoder<a class="headerlink" href="#autoencoder" title="Permanent link">¶</a></h4>
<p>Learn a lower-dimensional feature representation with unsupervised approaches.</p>
<p><span class="arithmatex">\(x\rightarrow z\)</span> : Dimension reduction for input features.</p>
<p><span class="arithmatex">\(z\rightarrow \hat{x}\)</span> : Reconstruct input features.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-autoencoder.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-autoencoder.png"/></a></p>
<p>After training, we throw the decoder away and use the encoder for transferring.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-autoencoder_transfer.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-autoencoder_transfer.png"/></a></p>
<p><strong>For generative models, there is a problem:</strong></p>
<p>We can’t generate new images from an autoencoder because we don’t know the space of <span class="arithmatex">\(z\)</span>.</p>
<h4 id="variational-autoencoder_1">Variational Autoencoder<a class="headerlink" href="#variational-autoencoder_1" title="Permanent link">¶</a></h4>
<h5 id="character-description">Character Description<a class="headerlink" href="#character-description" title="Permanent link">¶</a></h5>
<p><span class="arithmatex">\(X\)</span> : Images. <strong>(random variable)</strong></p>
<p><span class="arithmatex">\(Z\)</span> : Latent representations. <strong>(random variable)</strong></p>
<p><span class="arithmatex">\(P(X)\)</span> : True distribution of all training images <span class="arithmatex">\(X\)</span>.</p>
<p><span class="arithmatex">\(P(Z)\)</span> : True distribution of all latent representations <span class="arithmatex">\(Z\)</span>.</p>
<p><span class="arithmatex">\(P(X|Z)\)</span> : True <strong>posterior</strong> distribution of all images <span class="arithmatex">\(X\)</span> with condition <span class="arithmatex">\(Z\)</span>.</p>
<p><span class="arithmatex">\(P(Z|X)\)</span> : True <strong>prior</strong> distribution of all latent representations <span class="arithmatex">\(Z\)</span> with condition <span class="arithmatex">\(X\)</span>.</p>
<p><span class="arithmatex">\(Q(Z|X)\)</span> : Approximated <strong>prior</strong> distribution of all latent representations <span class="arithmatex">\(Z\)</span> with condition <span class="arithmatex">\(X\)</span>.</p>
<p><span class="arithmatex">\(x\)</span> : A specific image.</p>
<p><span class="arithmatex">\(z\)</span> : A specific latent representation.</p>
<p><span class="arithmatex">\(\theta\)</span>: Learned parameters in decoder network.</p>
<p><span class="arithmatex">\(\phi\)</span>: Learned parameters in encoder network.</p>
<p><span class="arithmatex">\(p_\theta(x)\)</span> : Probability that <span class="arithmatex">\(x\sim P(X)\)</span>.</p>
<p><span class="arithmatex">\(p_\theta(z)\)</span> : Probability that <span class="arithmatex">\(z\sim P(Z)\)</span>.</p>
<p><span class="arithmatex">\(p_\theta(x|z)\)</span> : Probability that <span class="arithmatex">\(x\sim P(X|Z)\)</span>.</p>
<p><span class="arithmatex">\(p_\theta(z|x)\)</span> : Probability that <span class="arithmatex">\(z\sim P(Z|X)\)</span>.</p>
<p><span class="arithmatex">\(q_\phi(z|x)\)</span> : Probability that <span class="arithmatex">\(z\sim Q(Z|X)\)</span>.</p>
<h5 id="decoder">Decoder<a class="headerlink" href="#decoder" title="Permanent link">¶</a></h5>
<p><strong>Objective:</strong></p>
<p>Generate new images from <span class="arithmatex">\(\mathscr{z}\)</span>.</p>
<ol>
<li>Generate a value <span class="arithmatex">\(z^{(i)}\)</span> from the prior distribution <span class="arithmatex">\(P(Z)\)</span>.</li>
<li>Generate a value <span class="arithmatex">\(x^{(i)}\)</span> from the conditional distribution <span class="arithmatex">\(P(X|Z)\)</span>.</li>
</ol>
<p><strong>Lemma:</strong></p>
<p>Any distribution in <span class="arithmatex">\(d\)</span> dimensions can be generated by taking a set of <span class="arithmatex">\(d\)</span> variables that are <strong>normally distributed</strong> and mapping them through a sufficiently complicated function. (source: <a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a>, Page 6)</p>
<p><strong>Solutions:</strong></p>
<ol>
<li>Choose prior distribution <span class="arithmatex">\(P(Z)\)</span> to be a simple distribution, for example <span class="arithmatex">\(P(Z)\sim N(0,1)\)</span>.</li>
<li>Learn the conditional distribution <span class="arithmatex">\(P(X|Z)\)</span> through a neural network (decoder) with parameter <span class="arithmatex">\(\theta\)</span>. </li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_decoder.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_decoder.png"/></a></p>
<h5 id="encoder">Encoder<a class="headerlink" href="#encoder" title="Permanent link">¶</a></h5>
<p><strong>Objective:</strong></p>
<p>Learn <span class="arithmatex">\(\mathscr{z}\)</span> with training images.</p>
<p><strong>Given:</strong> (From the decoder, we can deduce the following probabilities.)</p>
<ol>
<li><em>data likelihood:</em> <span class="arithmatex">\(p_\theta(x)=\int p_\theta(x|z)p_\theta(z)dz\)</span>.</li>
<li><em>posterior density:</em> <span class="arithmatex">\(p_\theta(z|x)=\frac{p_\theta(x|z)p_\theta(z)}{p_\theta(x)}=\frac{p_\theta(x|z)p_\theta(z)}{\int p_\theta(x|z)p_\theta(z)dz}\)</span>.</li>
</ol>
<p><strong>Problem:</strong></p>
<p>Both <span class="arithmatex">\(p_\theta(x)\)</span> and <span class="arithmatex">\(p_\theta(z|x)\)</span> are intractable. (can't be optimized directly as they contain <em>integral operation</em>)</p>
<p><strong>Solution:</strong></p>
<p>Learn <span class="arithmatex">\(Q(Z|X)\)</span> to approximate the true posterior <span class="arithmatex">\(P(Z|X)\)</span>.</p>
<p>Use <span class="arithmatex">\(q_\phi(z|x)\)</span> in place of <span class="arithmatex">\(p_\theta(z|x)\)</span>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_encoder.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_encoder.png"/></a></p>
<h5 id="variational-autoencoder-combination-of-encoder-and-decoder">Variational Autoencoder (Combination of Encoder and Decoder)<a class="headerlink" href="#variational-autoencoder-combination-of-encoder-and-decoder" title="Permanent link">¶</a></h5>
<p><strong>Objective:</strong></p>
<p>Maximize <span class="arithmatex">\(p_\theta(x)\)</span> for all <span class="arithmatex">\(x^{(i)}\)</span> in the training set.</p>
<p>$$
\begin{aligned}
\log p_\theta\big(x^{(i)}\big)&amp;=\mathbb{E}<em>{z\sim q</em>\phi\big(z|x^{(i)}\big)}\Big[\log p_\theta\big(x^{(i)}\big)\Big]\</p>
<p>&amp;=\mathbb{E}<em>z\Bigg[\log\frac{p</em>\theta\big(x^{(i)}|z\big)p_\theta\big(z\big)}{p_\theta\big(z|x^{(i)}\big)}\Bigg]\quad\text{(Bayes' Rule)}\</p>
<p>&amp;=\mathbb{E}<em>z\Bigg[\log\frac{p</em>\theta\big(x^{(i)}|z\big)p_\theta\big(z\big)}{p_\theta\big(z|x^{(i)}\big)}\frac{q_\phi\big(z|x^{(i)}\big)}{q_\phi\big(z|x^{(i)}\big)}\Bigg]\quad\text{(Multiply by Constant)}\</p>
<p>&amp;=\mathbb{E}<em>z\Big[\log p</em>\theta\big(x^{(i)}|z\big)\Big]-\mathbb{E}<em>z\Bigg[\log\frac{q</em>\phi\big(z|x^{(i)}\big)}{p_\theta\big(z\big)}\Bigg]+\mathbb{E}<em>z\Bigg[\log\frac{p</em>\theta\big(z|x^{(i)}\big)}{q_\phi\big(z|x^{(i)}\big)}\Bigg]\quad\text{(Logarithm)}\</p>
<p>&amp;=\mathbb{E}<em>z\Big[\log p</em>\theta\big(x^{(i)}|z\big)\Big]-D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]+D_{\text{KL}}\Big[p_\theta\big(z|x^{(i)}\big)||q_\phi\big(z|x^{(i)}\big)\Big]\quad\text{(KL Divergence)}
\end{aligned}
$$</p>
<p><strong>Analyze the Formula by Term:</strong></p>
<p><span class="arithmatex">\(\mathbb{E}_z\Big[\log p_\theta\big(x^{(i)}|z\big)\Big]\)</span>: Decoder network gives <span class="arithmatex">\(p_\theta\big(x^{(i)}|z\big)\)</span>, can compute estimate of this term through sampling.</p>
<p><span class="arithmatex">\(D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]\)</span>: This KL term (between Gaussians for encoder and <span class="arithmatex">\(z\)</span> prior) has nice closed-form solution!</p>
<p><span class="arithmatex">\(D_{\text{KL}}\Big[p_\theta\big(z|x^{(i)}\big)||q_\phi\big(z|x^{(i)}\big)\Big]\)</span>: The part <span class="arithmatex">\(p_\theta\big(z|x^{(i)}\big)\)</span> is intractable. <strong>However, we know KL divergence always <span class="arithmatex">\(\ge0\)</span>.</strong></p>
<p><strong>Tractable Lower Bound:</strong></p>
<p>We can maximize the lower bound of that formula.</p>
<p>As <span class="arithmatex">\(D_{\text{KL}}\Big[p_\theta\big(z|x^{(i)}\big)||q_\phi\big(z|x^{(i)}\big)\Big]\ge0\)</span> , we can deduce that:</p>
<p>$$
\begin{aligned}
\log p_\theta\big(x^{(i)}\big)&amp;=\mathbb{E}<em>z\Big[\log p</em>\theta\big(x^{(i)}|z\big)\Big]-D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]+D_{\text{KL}}\Big[p_\theta\big(z|x^{(i)}\big)||q_\phi\big(z|x^{(i)}\big)\Big]\</p>
<p>&amp;\ge\mathbb{E}<em>z\Big[\log p</em>\theta\big(x^{(i)}|z\big)\Big]-D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]
\end{aligned}
$$</p>
<p>So the loss function <span class="arithmatex">\(\mathcal{L}\big(x^{(i)},\theta,\phi\big)=-\mathbb{E}_z\Big[\log p_\theta\big(x^{(i)}|z\big)\Big]+D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]\)</span>.</p>
<p><span class="arithmatex">\(\mathbb{E}_z\Big[\log p_\theta\big(x^{(i)}|z\big)\Big]\)</span>: <strong><em>Decoder</em></strong>, reconstruct the input data. </p>
<p><span class="arithmatex">\(D_{\text{KL}}\Big[q_\phi\big(z|x^{(i)}\big)||p_\theta\big(z\big)\Big]\)</span>: <strong><em>Encoder</em></strong>, make approximate posterior distribution close to prior.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_combination.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-var_autoencoder_combination.png"/></a></p>
<h3 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)<a class="headerlink" href="#generative-adversarial-networks-gans" title="Permanent link">¶</a></h3>
<h4 id="motivation-modeling">Motivation &amp; Modeling<a class="headerlink" href="#motivation-modeling" title="Permanent link">¶</a></h4>
<p><strong>Objective:</strong> Not modeling any explicit density function.</p>
<p><strong>Problem:</strong> Want to sample from complex, high-dimensional training distribution. <strong>No direct way to do this!</strong></p>
<p><strong>Solution:</strong> Sample from a simple distribution, e.g. <strong>random noise</strong>. Learn the transformation to training distribution.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage1.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage1.png"/></a></p>
<p><strong>Problem:</strong> We can't learn the <strong>mapping relation</strong> between sample <span class="arithmatex">\(z\)</span> and training images.</p>
<p><strong>Solution:</strong> Use a <strong>discriminator network</strong> to tell whether the generate image is within data distribution or not.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage2.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage2.png"/></a></p>
<p><strong>Discriminator network:</strong> Try to distinguish between real and fake images.</p>
<p><strong>Generator network:</strong> Try to fool the discriminator by generating real-looking images.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage3.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_stage3.png"/></a></p>
<p><span class="arithmatex">\(x\)</span> : Real data.</p>
<p><span class="arithmatex">\(y\)</span> : Fake data, which is generated by the generator network. <span class="arithmatex">\(y=G_{\theta_g}(z)\)</span>.</p>
<p><span class="arithmatex">\(D_{\theta_d}(x)\)</span> : Discriminator score, which is the likelihood of real image. <span class="arithmatex">\(D_{\theta_d}(x)\in[0,1]\)</span>.</p>
<p><strong>Objective of discriminator network:</strong></p>
<p><span class="arithmatex">\(\max_{\theta_d}\bigg[\mathbb{E}_x\Big(\log D_{\theta_d}(x)\Big)+\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p>
<p><strong>Objective of generator network:</strong></p>
<p><span class="arithmatex">\(\min_{\theta_g}\max_{\theta_d}\bigg[\mathbb{E}_x\Big(\log D_{\theta_d}(x)\Big)+\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p>
<h4 id="training-strategy">Training Strategy<a class="headerlink" href="#training-strategy" title="Permanent link">¶</a></h4>
<p>Two combine this two networks together, we can train them alternately:</p>
<ol>
<li>Gradient <strong>ascent</strong> on discriminator.</li>
</ol>
<p><span class="arithmatex">\(\max_{\theta_d}\bigg[\mathbb{E}_x\Big(\log D_{\theta_d}(x)\Big)+\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p>
<ol start="2">
<li>Gradient <strong>descent</strong> on generator.</li>
</ol>
<p><span class="arithmatex">\(\min_{\theta_g}\bigg[\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p>
<p>However, the gradient of generator decreases with the value itself, making it <strong>hard to optimize</strong>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_gradient.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/13-gan_gradient.png"/></a></p>
<p>So we replace <span class="arithmatex">\(\log\big(1-D_{\theta_d}(y)\big)\)</span> with <span class="arithmatex">\(-\log D_{\theta_d}(y)\)</span>, and use gradient ascent instead.</p>
<ol>
<li>Gradient <strong>ascent</strong> on discriminator.</li>
</ol>
<p><span class="arithmatex">\(\max_{\theta_d}\bigg[\mathbb{E}_x\Big(\log D_{\theta_d}(x)\Big)+\mathbb{E}_{z\sim p(z)}\Big(\log\big(1-D_{\theta_d}(y)\big)\Big)\bigg]\)</span></p>
<ol start="2">
<li>Gradient <strong>ascent</strong> on generator.</li>
</ol>
<p><span class="arithmatex">\(\max_{\theta_g}\bigg[\mathbb{E}_{z\sim p(z)}\Big(\log D_{\theta_d}(y)\Big)\bigg]\)</span></p>
<h4 id="summary_3">Summary<a class="headerlink" href="#summary_3" title="Permanent link">¶</a></h4>
<p><strong>Pros:</strong> Beautiful, state-of-the-art samples!</p>
<p><strong>Cons:</strong> </p>
<ol>
<li>Trickier / more unstable to train.</li>
<li>Can’t solve inference queries such as <span class="arithmatex">\(p(x), p(z|x)\)</span>.</li>
</ol>
<h2 id="14-self-supervised-learning">14 - Self-supervised Learning<a class="headerlink" href="#14-self-supervised-learning" title="Permanent link">¶</a></h2>
<p><strong>Aim:</strong> Solve “pretext” tasks that produce good features for downstream tasks.</p>
<p><strong>Application:</strong></p>
<ol>
<li>Learn a feature extractor from pretext tasks. <strong>(self-supervised)</strong></li>
<li>Attach a shallow network on the feature extractor.</li>
<li>Train the shallow network on target task with small amount of labeled data. <strong>(supervised)</strong></li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-self_supervised_learning.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-self_supervised_learning.png"/></a></p>
<h3 id="pretext-tasks">Pretext Tasks<a class="headerlink" href="#pretext-tasks" title="Permanent link">¶</a></h3>
<p>Labels are generated automatically.</p>
<h4 id="rotation">Rotation<a class="headerlink" href="#rotation" title="Permanent link">¶</a></h4>
<p>Train a classifier on randomly rotated images.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_rotation.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_rotation.png"/></a></p>
<h4 id="rearrangement">Rearrangement<a class="headerlink" href="#rearrangement" title="Permanent link">¶</a></h4>
<p>Train a classifier on randomly shuffled image pieces.</p>
<p>Predict the location of image pieces.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_rearrangement.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_rearrangement.png"/></a></p>
<h4 id="inpainting">Inpainting<a class="headerlink" href="#inpainting" title="Permanent link">¶</a></h4>
<p>Mask part of the image, train a network to predict the masked area.</p>
<p>Method referencing <a href="https://arxiv.org/pdf/1604.07379.pdf">Context Encoders: Feature Learning by Inpainting</a>.</p>
<p>Combine two types of loss together to get better performance:</p>
<ol>
<li><strong>Reconstruction loss (L2 loss):</strong> Used for reconstructing global features.</li>
<li><strong>Adversarial loss:</strong> Used for generating texture features.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_inpainting.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_inpainting.png"/></a></p>
<h4 id="coloring">Coloring<a class="headerlink" href="#coloring" title="Permanent link">¶</a></h4>
<p>Transfer between greyscale images and colored images.</p>
<p><strong>Cross-channel predictions for images:</strong> <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Split-Brain_Autoencoders_Unsupervised_CVPR_2017_paper.pdf">Split-Brain Autoencoders</a>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_coloring_sb_ae.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_coloring_sb_ae.png"/></a></p>
<p><strong>Video coloring:</strong> Establish mappings between reference and target frames in a learned feature space. <a href="https://arxiv.org/abs/1806.09594">Tracking Emerges by Colorizing Videos</a>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_coloring_video.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-pretext_coloring_video.png"/></a></p>
<h4 id="summary-for-pretext-tasks">Summary for Pretext Tasks<a class="headerlink" href="#summary-for-pretext-tasks" title="Permanent link">¶</a></h4>
<ol>
<li>Pretext tasks focus on <strong>“visual common sense”</strong>.</li>
<li>The models are forced learn good features about natural images.</li>
<li>We <strong>don’t</strong> care about the performance of these <strong>pretext tasks</strong>. </li>
</ol>
<p>What we care is the performance of <strong>downstream tasks</strong>.</p>
<h4 id="problems-of-specific-pretext-tasks">Problems of Specific Pretext Tasks<a class="headerlink" href="#problems-of-specific-pretext-tasks" title="Permanent link">¶</a></h4>
<ol>
<li>Coming up with <strong>individual</strong> pretext tasks is tedious.</li>
<li>The learned representations may <strong>not be general</strong>.</li>
</ol>
<p><strong>Intuitive Solution:</strong> Contrastive Learning.</p>
<h3 id="contrastive-representation-learning">Contrastive Representation Learning<a class="headerlink" href="#contrastive-representation-learning" title="Permanent link">¶</a></h3>
<p><strong>Local additional references:</strong> <a href="....\DL\Contrastive Learning\Contrastive Learning.md">Contrastive Learning.md</a>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive.png"/></a></p>
<p><strong>Objective:</strong></p>
<p>Given a chosen score function <span class="arithmatex">\(s\)</span>, we aim to learn an encoder function <span class="arithmatex">\(f\)</span> that yields:</p>
<ol>
<li>For each sample <span class="arithmatex">\(x\)</span>, increase the similarity <span class="arithmatex">\(s\big(f(x),f(x^+)\big)\)</span> between <span class="arithmatex">\(x\)</span> and positive samples <span class="arithmatex">\(x^+\)</span>.</li>
<li>Finally we want <span class="arithmatex">\(s\big(f(x),f(x^+)\big)\gg s\big(f(x),f(x^-)\big)\)</span>.</li>
</ol>
<p><strong>Loss Function:</strong> </p>
<p>Given <span class="arithmatex">\(1\)</span> positive sample and <span class="arithmatex">\(N-1\)</span> negative samples:</p>
<table>
<thead>
<tr>
<th>InfoNCE Loss</th>
<th>Cross Entropy Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(\begin{aligned}\mathcal{L}=-\mathbb{E}_X\Bigg[\log\frac{\exp{s\big(f(x),f(x^+)\big)}}{\exp{s\big(f(x),f(x^+)\big)}+\sum_{j=1}^{N-1}\exp{s\big(f(x),f(x^+)\big)}}\Bigg]\\\end{aligned}\)</span></td>
<td><span class="arithmatex">\(\begin{aligned}\mathcal{L}&amp;=-\sum_{i=1}^Np(x_i)\log q(x_i)\\&amp;=-\mathbb{E}_X\big[\log q(x)\big]\\&amp;=-\mathbb{E}_X\Bigg[\log\frac{\exp(x)}{\sum_{j=1}^N\exp(x_j)}\Bigg]\end{aligned}\)</span></td>
</tr>
</tbody>
</table>
<p>The <em>InfoNCE Loss</em> is a lower bound on the <em>mutual information</em> between <span class="arithmatex">\(f(x)\)</span> and <span class="arithmatex">\(f(x^+)\)</span>:</p>
<p><span class="arithmatex">\(\text{MI}\big[f(x),f(x^+)\big]\ge\log(N)-\mathcal{L}\)</span></p>
<p>The <em>larger</em> the negative sample size <span class="arithmatex">\(N\)</span>, the <em>tighter</em> the bound.</p>
<p>So we use <span class="arithmatex">\(N-1\)</span> negative samples.</p>
<h4 id="instance-contrastive-learning">Instance Contrastive Learning<a class="headerlink" href="#instance-contrastive-learning" title="Permanent link">¶</a></h4>
<h5 id="simclr"><a href="https://arxiv.org/pdf/2002.05709.pdf">SimCLR</a><a class="headerlink" href="#simclr" title="Permanent link">¶</a></h5>
<p>Use a projection function <span class="arithmatex">\(g(\cdot)\)</span> to project features to a space where contrastive learning is applied.</p>
<p>The extra projection contributes a lot to the final performance.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_simclr_frame.jpg"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_simclr_frame.jpg"/></a></p>
<p><strong>Score Function:</strong> Cos similarity <span class="arithmatex">\(s(u,v)=\frac{u^Tv}{||u||||v||}\\\)</span>.</p>
<p><strong>Positive Pair:</strong> Pair of augmented data.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_simclr_algo.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_simclr_algo.png"/></a></p>
<h5 id="momentum-contrastive-learning-moco"><a href="https://arxiv.org/pdf/1911.05722.pdf">Momentum Contrastive Learning (MoCo)</a><a class="headerlink" href="#momentum-contrastive-learning-moco" title="Permanent link">¶</a></h5>
<p>There are mainly <span class="arithmatex">\(3\)</span> training strategy in contrastive learning:</p>
<ol>
<li><em>end-to-end:</em> Keys are updated together with queries, e.g. <strong><em>SimCLR</em></strong>.</li>
</ol>
<p><strong>(limited by GPU size)</strong></p>
<ol start="2">
<li><em>memory bank:</em> Store last-time keys for sampling.</li>
</ol>
<p><strong>(inconsistency between <span class="arithmatex">\(q\)</span> and <span class="arithmatex">\(k\)</span>)</strong></p>
<ol start="3">
<li><em><strong>MoCo</strong>:</em> Use momentum methods to encode keys.</li>
</ol>
<p><strong>(combination of <em>end-to-end</em> &amp; <em>memory bank</em>)</strong></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_moco_cate.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_moco_cate.png"/></a></p>
<p><strong>Key differences to SimCLR:</strong></p>
<ol>
<li>Keep a running <strong>queue</strong> of keys (negative samples).</li>
<li>Compute gradients and update the encoder <strong>only through the queries</strong>.</li>
<li>Decouple min-batch size with the number of keys: can support <strong>a large number of negative samples</strong>.</li>
<li>The key encoder is <strong>slowly progressing</strong> through the momentum update rules:</li>
</ol>
<p><span class="arithmatex">\(\theta_k\leftarrow m\theta_k+(1-m)\theta_q\)</span></p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_moco_algo.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_moco_algo.png"/></a></p>
<h4 id="sequence-contrastive-learning">Sequence Contrastive Learning<a class="headerlink" href="#sequence-contrastive-learning" title="Permanent link">¶</a></h4>
<h5 id="contrastive-predictive-coding-cpc">Contrastive Predictive Coding (CPC)<a class="headerlink" href="#contrastive-predictive-coding-cpc" title="Permanent link">¶</a></h5>
<p><strong>Contrastive:</strong> Contrast between “right” and “wrong” sequences using contrastive learning.</p>
<p><strong>Predictive:</strong> The model has to <em>predict</em> future patterns given the current context.</p>
<p><strong>Coding:</strong> The model learns useful <em>feature vectors</em>, or “code”, for downstream tasks, similar to other self-supervised methods.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_cpc.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_cpc.png"/></a></p>
<h4 id="other-examples-frontier">Other Examples (Frontier)<a class="headerlink" href="#other-examples-frontier" title="Permanent link">¶</a></h4>
<h5 id="contrastive-language-image-pre-training-clip">Contrastive Language Image Pre-training (CLIP)<a class="headerlink" href="#contrastive-language-image-pre-training-clip" title="Permanent link">¶</a></h5>
<p>Contrastive learning between image and natural language sentences.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_clip.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/14-contrastive_clip.png"/></a></p>
<h2 id="15-low-level-vision">15 - Low-Level Vision<a class="headerlink" href="#15-low-level-vision" title="Permanent link">¶</a></h2>
<p>Pass...</p>
<h2 id="16-3d-vision">16 - 3D Vision<a class="headerlink" href="#16-3d-vision" title="Permanent link">¶</a></h2>
<h3 id="representation">Representation<a class="headerlink" href="#representation" title="Permanent link">¶</a></h3>
<h4 id="explicit-vs-implicit">Explicit vs Implicit<a class="headerlink" href="#explicit-vs-implicit" title="Permanent link">¶</a></h4>
<p><strong>Explicit:</strong> Easy to sample examples, hard to do inside/outside check.</p>
<p><strong>Implicit:</strong> Hard to sample examples, easy to do inside/outside check.</p>
<table>
<thead>
<tr>
<th></th>
<th>Non-parametric</th>
<th>Parametric</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Explicit</strong></td>
<td>Points.<br/>Meshes.</td>
<td>Splines.<br/>Subdivision Surfaces.</td>
</tr>
<tr>
<td><strong>Implicit</strong></td>
<td>Level Sets.<br/>Voxels.</td>
<td>Algebraic Surfaces.<br/>Constructive Solid Geometry.</td>
</tr>
</tbody>
</table>
<h4 id="point-clouds">Point Clouds<a class="headerlink" href="#point-clouds" title="Permanent link">¶</a></h4>
<p>The simplest representation.</p>
<p>Collection of <span class="arithmatex">\((x,y,z)\)</span> coordinates.</p>
<p><strong>Cons:</strong></p>
<ol>
<li>Difficult to draw in under-sampled regions.</li>
<li>No simplification or subdivision.</li>
<li>No direction smooth rendering.</li>
<li>No topological information.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_point_clouds.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_point_clouds.png"/></a></p>
<h4 id="polygonal-meshes">Polygonal Meshes<a class="headerlink" href="#polygonal-meshes" title="Permanent link">¶</a></h4>
<p>Collection of vertices <span class="arithmatex">\(v\)</span> and edges <span class="arithmatex">\(e\)</span>.</p>
<p><strong>Pros:</strong></p>
<ol>
<li>Can apply downsampling or upsampling on meshes.</li>
<li>Error decreases by <span class="arithmatex">\(O(n^2)\)</span> while meshes increase by <span class="arithmatex">\(O(n)\)</span>.</li>
<li>Can approximate arbitrary topology.</li>
<li>Efficient rendering.</li>
</ol>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_poly.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_poly.png"/></a></p>
<h4 id="splines">Splines<a class="headerlink" href="#splines" title="Permanent link">¶</a></h4>
<p>Use specific functions to approximate the surface. (e.g. Bézier Curves)</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_bezier.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_bezier.png"/></a></p>
<h4 id="algebraic-surfaces">Algebraic Surfaces<a class="headerlink" href="#algebraic-surfaces" title="Permanent link">¶</a></h4>
<p>Use specific functions to represent the surface.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_algebra.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_algebra.png"/></a></p>
<h4 id="constructive-solid-geometry">Constructive Solid Geometry<a class="headerlink" href="#constructive-solid-geometry" title="Permanent link">¶</a></h4>
<p>Combine implicit geometry with Boolean operations.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_boolean.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_boolean.png"/></a></p>
<h4 id="level-sets">Level Sets<a class="headerlink" href="#level-sets" title="Permanent link">¶</a></h4>
<p>Store a grim of values to approximate the function.</p>
<p>Surface is found where interpolated value equals to <span class="arithmatex">\(0\)</span>.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_level_set.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_level_set.png"/></a></p>
<h4 id="voxels">Voxels<a class="headerlink" href="#voxels" title="Permanent link">¶</a></h4>
<p>Binary thresholding the volumetric grid.</p>
<p><a class="glightbox" data-desc-position="bottom" data-height="auto" data-type="image" data-width="80%" href="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_binary.png"><img alt="" src="https://raw.githubusercontent.com/WncFht/picture/main/picture/16-representation_binary.png"/></a></p>
<h3 id="ai-3d">AI + 3D<a class="headerlink" href="#ai-3d" title="Permanent link">¶</a></h3>
<p>Pass...</p>
<div id="__comments"></div>
<!-- <h3>颜色主题调整</h3>
    <div class="tx-switch">
    <button class="button1" data-md-color-primary="red" style="background-color:red">red</button>
    <button class="button1" data-md-color-primary="pink" style="background-color:pink;color:black">pink</button>
    <button class="button1" data-md-color-primary="purple" style="background-color:purple">purple</button>
    <button class="button1" data-md-color-primary="indigo" style="background-color:indigo">indigo</button>
    <button class="button1" data-md-color-primary="blue" style="background-color:blue">blue</button>
    <button class="button1" data-md-color-primary="cyan" style="background-color:cyan;color:black">cyan</button>
    <button class="button1" data-md-color-primary="teal" style="background-color:teal">teal</button>
    <button class="button1" data-md-color-primary="green" style="background-color:green">green</button>
    <button class="button1" data-md-color-primary="lime" style="background-color:lime;color:black">lime</button>
    <button class="button1" data-md-color-primary="orange" style="background-color:orange;color:black">orange</button>
    <button class="button1" data-md-color-primary="brown" style="background-color:brown;border-radius=3px">brown</button>
    <button class="button1" data-md-color-primary="grey" style="background-color:grey">grey</button>
    <button class="button1" data-md-color-primary="black" style="background-color:black">black</button>
    <button class="button1" data-md-color-primary="white" style="background-color:white;color:black">white</button>
    </div> -->
<!-- Giscus -->
<script async="" crossorigin="anonymous" data-category="General" data-category-id="DIC_kwDONBPLqs4Cja0c" data-emit-metadata="0" data-input-position="bottom" data-lang="zh-CN" data-mapping="pathname" data-reactions-enabled="1" data-repo="WncFht/notes" data-repo-id="R_kgDONBPLqg" data-strict="0" data-theme="preferred_color_scheme" src="https://giscus.app/client.js">
</script>
<script>
    var buttons = document.querySelectorAll("button[data-md-color-primary]")
    buttons.forEach(function(button) {
            button.addEventListener("click", function() {
            var attr = this.getAttribute("data-md-color-primary")
            document.body.setAttribute("data-md-color-primary", attr)
            localStorage.setItem("data-md-color-primary",attr);
            })
    })
    </script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")
</script>
<script>
    var giscus = document.querySelector("script[src*=giscus]")
    
    /* Set palette on initial load */
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate" ? "dark" : "light"
        giscus.setAttribute("data-theme", theme) 
    }
    
    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function() {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
            var theme = palette.color.scheme === "slate" ? "dark" : "light"
    
            /* Instruct Giscus to change theme */
            var frame = document.querySelector(".giscus-frame")
            frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
            )
        }
        })
    })
    </script>
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  回到页面顶部
</button>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
        Copyright © 2024 <a href="https://github.com/WncFht" rel="noopener" target="_blank">WncFht</a>
</div>
    
    
      Powered by
      <a href="https://www.mkdocs.org/" rel="noopener" target="_blank">
        MkDocs
      </a>
      with theme
      <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
        Material
      </a>
      modified by
      <a href="https://github.com/WncFht" rel="noopener" target="_blank">
        WncFht
      </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/WncFht" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.annotate", "content.action.view", "navigation.tracking", "navigation.tabs", "navigation.indexes", "navigation.top", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
<script src="../../../assets/javascripts/bundle.83f73b43.min.js"></script>
<script src="../../../js/katex.js"></script>
<script src="../../../js/mathjax.js"></script>
<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml-full.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-svg-full.js"></script>
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>